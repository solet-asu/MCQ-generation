{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0af7df",
   "metadata": {},
   "source": [
    " Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89290e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e3800",
   "metadata": {},
   "source": [
    "Prompt Loading From Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD EXISTING PROMPT FILES FROM YOUR PROMPTS FOLDER\n",
    "# =============================================================================\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "\n",
    "# Enable nested async for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# OpenAI Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Load your existing YAML prompt files\n",
    "def load_prompts():\n",
    "    \"\"\"Load all prompt templates from your existing YAML files.\"\"\"\n",
    "    try:\n",
    "        with open(\"../prompts/syntactic_analyzer_prompts.yaml\", 'r') as f:\n",
    "            syntactic_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        with open(\"../prompts/candidate_generation_prompts.yaml\", 'r') as f:\n",
    "            generation_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        with open(\"../prompts/candidate_selection_prompts.yaml\", 'r') as f:\n",
    "            selection_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        print(\" All prompt files loaded successfully from your prompts folder\")\n",
    "        return syntactic_prompts, generation_prompts, selection_prompts\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" Error loading prompt files: {e}\")\n",
    "        print(\"Please ensure the YAML files exist in your prompts/ folder\")\n",
    "        raise\n",
    "\n",
    "# Load your existing prompts\n",
    "syntactic_prompts, generation_prompts, selection_prompts = load_prompts()\n",
    "\n",
    "print(\"ASU LEI Team - Option Shortening Workflow\")\n",
    "print(\"Using existing prompts from prompts/ folder\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc36a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c250588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tf-keras --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d840c",
   "metadata": {},
   "source": [
    "SETUP AND DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASU LEI Team - Option Shortening Workflow Implementation\n",
    "# Complete 5-Step Workflow as per user's Specifications\n",
    "# Research Assistant: Shubham \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import yaml\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Enable nested async for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"ASU LEI Team - Option Shortening Workflow\")\n",
    "print(\"Implementing user's 5-Step Model\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Set up paths\n",
    "project_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\n",
    "prompts_dir = project_root / \"prompts\"\n",
    "data_dir = project_root / \"database\"\n",
    "\n",
    "# OpenAI Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"Please set your API key: export OPENAI_API_KEY='your-key'\")\n",
    "    OPENAI_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load prompts from YAML files\n",
    "def load_prompts():\n",
    "    \"\"\"Load all prompt templates from YAML files.\"\"\"\n",
    "    try:\n",
    "        with open(prompts_dir / \"syntactic_analyzer_prompts.yaml\", 'r') as f:\n",
    "            syntactic_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        with open(prompts_dir / \"candidate_generation_prompts.yaml\", 'r') as f:\n",
    "            generation_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        with open(prompts_dir / \"candidate_selection_prompts.yaml\", 'r') as f:\n",
    "            selection_prompts = yaml.safe_load(f)\n",
    "        \n",
    "        print(\" All prompt files loaded successfully\")\n",
    "        return syntactic_prompts, generation_prompts, selection_prompts\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" Error loading prompt files: {e}\")\n",
    "        print(\"Please ensure all YAML files are in the prompts/ directory\")\n",
    "        raise\n",
    "\n",
    "syntactic_prompts, generation_prompts, selection_prompts = load_prompts()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PARSING FUNCTIONS (FIXED LOGIC)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_mcq_components(question_text: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract question stem and options from MCQ text.\n",
    "    \n",
    "    CRITICAL FIX: This function now correctly processes the Question column\n",
    "    which contains the full MCQ text with options A), B), C), D)\n",
    "    \n",
    "    Args:\n",
    "        question_text (str): Full question text from Question column\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: (question_stem, [option_A, option_B, option_C, option_D])\n",
    "    \"\"\"\n",
    "    if pd.isna(question_text) or not question_text.strip():\n",
    "        return \"\", [None, None, None, None]\n",
    "    \n",
    "    # Split by lines and clean\n",
    "    lines = [line.strip() for line in question_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Find where options start (look for first A), B), C), or D))\n",
    "    question_lines = []\n",
    "    options_start_idx = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^[A-D]\\)', line):\n",
    "            options_start_idx = i\n",
    "            break\n",
    "        question_lines.append(line)\n",
    "    \n",
    "    # Extract question stem\n",
    "    question_stem = ' '.join(question_lines).strip()\n",
    "    \n",
    "    # Extract options\n",
    "    if options_start_idx is None:\n",
    "        # No options found, return question as is\n",
    "        return question_stem, [None, None, None, None]\n",
    "    \n",
    "    options = [None, None, None, None]  # A, B, C, D\n",
    "    current_option_idx = None\n",
    "    \n",
    "    for line in lines[options_start_idx:]:\n",
    "        # Check if line starts with option marker\n",
    "        option_match = re.match(r'^([A-D])\\)\\s*(.*)$', line)\n",
    "        if option_match:\n",
    "            option_letter = option_match.group(1)\n",
    "            option_text = option_match.group(2)\n",
    "            current_option_idx = ord(option_letter) - ord('A')\n",
    "            options[current_option_idx] = option_text\n",
    "        elif current_option_idx is not None and line:\n",
    "            # Continue current option on next line\n",
    "            if options[current_option_idx]:\n",
    "                options[current_option_idx] += ' ' + line\n",
    "            else:\n",
    "                options[current_option_idx] = line\n",
    "    \n",
    "    # Clean up options\n",
    "    options = [opt.strip() if opt else None for opt in options]\n",
    "    \n",
    "    return question_stem, options\n",
    "\n",
    "def extract_correct_answer_letter(answer_text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the correct answer letter (A, B, C, or D) from answer text.\n",
    "    \n",
    "    Args:\n",
    "        answer_text (str): Answer text like \"A) Some answer text\"\n",
    "        \n",
    "    Returns:\n",
    "        str: Letter (A, B, C, or D) or None if not found\n",
    "    \"\"\"\n",
    "    if pd.isna(answer_text):\n",
    "        return None\n",
    "    \n",
    "    match = re.search(r'^([A-D])\\)?', answer_text.strip())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count words in text using robust word boundary detection.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to count words in\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of words\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    \n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return len(words)\n",
    "\n",
    "def validate_meaning_preservation(original_text: str, shortened_text: str, threshold: float = 0.75) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate that shortened text preserves the semantic meaning of original text.\n",
    "    \n",
    "    Args:\n",
    "        original_text (str): Original option text\n",
    "        shortened_text (str): Shortened option text\n",
    "        threshold (float): Minimum similarity score (0.75 = 75% similarity)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Validation results with similarity score and assessment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get embeddings for both texts\n",
    "        original_embedding = semantic_model.encode([original_text])\n",
    "        shortened_embedding = semantic_model.encode([shortened_text])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity_score = cosine_similarity(original_embedding, shortened_embedding)[0][0]\n",
    "        \n",
    "        # Assess meaning preservation\n",
    "        if similarity_score >= threshold:\n",
    "            preservation_status = \"EXCELLENT\"\n",
    "            preservation_message = \"Strong semantic similarity maintained\"\n",
    "        elif similarity_score >= 0.65:\n",
    "            preservation_status = \"GOOD\"\n",
    "            preservation_message = \"Adequate semantic similarity\"\n",
    "        elif similarity_score >= 0.50:\n",
    "            preservation_status = \"MODERATE\"\n",
    "            preservation_message = \"Some semantic drift detected\"\n",
    "        else:\n",
    "            preservation_status = \"POOR\"\n",
    "            preservation_message = \"Significant semantic change detected\"\n",
    "        \n",
    "        return {\n",
    "            'similarity_score': float(similarity_score),\n",
    "            'preservation_status': preservation_status,\n",
    "            'preservation_message': preservation_message,\n",
    "            'passes_threshold': similarity_score >= threshold,\n",
    "            'threshold_used': threshold\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'similarity_score': 0.0,\n",
    "            'preservation_status': \"ERROR\",\n",
    "            'preservation_message': f\"Validation failed: {str(e)}\",\n",
    "            'passes_threshold': False,\n",
    "            'threshold_used': threshold\n",
    "        }\n",
    "\n",
    "        #Validation work\n",
    "\n",
    "def validate_keyword_preservation(original_text: str, shortened_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Check if critical keywords from original text are preserved in shortened text.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Extract important words (nouns, adjectives, key verbs)\n",
    "    def extract_keywords(text):\n",
    "        # Simple keyword extraction - you can enhance this\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
    "        # Filter out common words\n",
    "        stop_words = {'that', 'this', 'with', 'from', 'they', 'have', 'will', 'been', 'were', 'said', 'each', 'which', 'their', 'them', 'than', 'many', 'some', 'what', 'would', 'make', 'like', 'into', 'time', 'more', 'very', 'when', 'come', 'could', 'also'}\n",
    "        return [w for w in words if w not in stop_words and len(w) >= 4]\n",
    "    \n",
    "    original_keywords = set(extract_keywords(original_text))\n",
    "    shortened_keywords = set(extract_keywords(shortened_text))\n",
    "    \n",
    "    if not original_keywords:\n",
    "        return {'preservation_rate': 1.0, 'missing_keywords': [], 'status': 'NO_KEYWORDS'}\n",
    "    \n",
    "    preserved_keywords = original_keywords.intersection(shortened_keywords)\n",
    "    missing_keywords = original_keywords - shortened_keywords\n",
    "    preservation_rate = len(preserved_keywords) / len(original_keywords)\n",
    "    \n",
    "    if preservation_rate >= 0.8:\n",
    "        status = \"EXCELLENT\"\n",
    "    elif preservation_rate >= 0.6:\n",
    "        status = \"GOOD\" \n",
    "    elif preservation_rate >= 0.4:\n",
    "        status = \"MODERATE\"\n",
    "    else:\n",
    "        status = \"POOR\"\n",
    "    \n",
    "    return {\n",
    "        'preservation_rate': preservation_rate,\n",
    "        'missing_keywords': list(missing_keywords),\n",
    "        'preserved_keywords': list(preserved_keywords),\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_shortening_quality(original_option: str, selected_candidate: str, \n",
    "                               semantic_validation: dict, keyword_validation: dict,\n",
    "                               min_semantic_threshold: float = 0.75,\n",
    "                               min_keyword_threshold: float = 0.6) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate if shortening meets quality standards or should be rejected.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Decision on whether to accept shortening or keep original\n",
    "        \"\"\"\n",
    "    \n",
    "        # Quality checks\n",
    "        semantic_pass = semantic_validation['similarity_score'] >= min_semantic_threshold\n",
    "        keyword_pass = keyword_validation['preservation_rate'] >= min_keyword_threshold\n",
    "        \n",
    "        # Check for critical meaning loss indicators\n",
    "        critical_keywords_lost = any(keyword in original_option.lower() for keyword in \n",
    "                                ['not', 'never', 'only', 'except', 'unless', 'without'] \n",
    "                                if keyword not in selected_candidate.lower())\n",
    "        \n",
    "        # Overall quality assessment\n",
    "        quality_score = (\n",
    "            semantic_validation['similarity_score'] * 0.6 +\n",
    "            keyword_validation['preservation_rate'] * 0.4\n",
    "        )\n",
    "        \n",
    "        # Decision logic\n",
    "        if quality_score >= 0.75 and semantic_pass and not critical_keywords_lost:\n",
    "            decision = \"ACCEPT\"\n",
    "            reason = \"High quality shortening preserves meaning\"\n",
    "        elif quality_score >= 0.65 and semantic_pass:\n",
    "            decision = \"ACCEPT_WITH_CAUTION\" \n",
    "            reason = \"Acceptable shortening with minor quality concerns\"\n",
    "        else:\n",
    "            decision = \"REJECT\"\n",
    "            reasons = []\n",
    "            if not semantic_pass:\n",
    "                reasons.append(f\"Low semantic similarity ({semantic_validation['similarity_score']:.2f})\")\n",
    "            if not keyword_pass:\n",
    "                reasons.append(f\"Poor keyword preservation ({keyword_validation['preservation_rate']:.1%})\")\n",
    "            if critical_keywords_lost:\n",
    "                reasons.append(\"Critical negation/qualifier words lost\")\n",
    "            reason = \"; \".join(reasons)\n",
    "        \n",
    "        return {\n",
    "            'decision': decision,\n",
    "            'quality_score': quality_score,\n",
    "            'reason': reason,\n",
    "            'semantic_pass': semantic_pass,\n",
    "            'keyword_pass': keyword_pass,\n",
    "            'critical_loss': critical_keywords_lost\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_parse_dataset(limit_rows: int = None):\n",
    "    \"\"\"\n",
    "    Load and parse the MCQ dataset.\n",
    "    \n",
    "    Args:\n",
    "        limit_rows (int): Limit to first N rows for testing (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Parsed and cleaned dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(data_dir / \"all_mcqs.csv\")\n",
    "        print(f\" Loaded dataset: {len(df)} rows\")\n",
    "        print(f\" Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Limit rows if specified\n",
    "        if limit_rows:\n",
    "            df = df.head(limit_rows)\n",
    "            print(f\"🔬 Using sample: {len(df)} rows for testing\")\n",
    "        \n",
    "        # Display sample structure\n",
    "        print(\"\\n Sample data structure:\")\n",
    "        for i, row in df.head(2).iterrows():\n",
    "            print(f\"Row {i}:\")\n",
    "            print(f\"  Subject: {row['Subject']}\")\n",
    "            print(f\"  Question: {row['Question'][:100]}...\")\n",
    "            print(f\"  Answer: {row['Answer']}\")\n",
    "            print()\n",
    "        \n",
    "        # CRITICAL: Parse from Question column, not Answer column\n",
    "        print(\"🔧 Parsing MCQ components...\")\n",
    "        parsing_results = df['Question'].apply(extract_mcq_components)\n",
    "        \n",
    "        # Create enhanced dataframe\n",
    "        df_parsed = df.copy()\n",
    "        df_parsed['question_stem'] = [result[0] for result in parsing_results]\n",
    "        df_parsed['options'] = [result[1] for result in parsing_results]\n",
    "        \n",
    "        # Extract correct answer letters from Answer column\n",
    "        df_parsed['correct_letter'] = df['Answer'].apply(extract_correct_answer_letter)\n",
    "        \n",
    "        # Create individual option columns\n",
    "        df_parsed['option_A'] = df_parsed['options'].apply(lambda x: x[0] if x else None)\n",
    "        df_parsed['option_B'] = df_parsed['options'].apply(lambda x: x[1] if x else None)\n",
    "        df_parsed['option_C'] = df_parsed['options'].apply(lambda x: x[2] if x else None)\n",
    "        df_parsed['option_D'] = df_parsed['options'].apply(lambda x: x[3] if x else None)\n",
    "        \n",
    "        # Count words for each option\n",
    "        df_parsed['words_A'] = df_parsed['option_A'].apply(count_words)\n",
    "        df_parsed['words_B'] = df_parsed['option_B'].apply(count_words)\n",
    "        df_parsed['words_C'] = df_parsed['option_C'].apply(count_words)\n",
    "        df_parsed['words_D'] = df_parsed['option_D'].apply(count_words)\n",
    "        \n",
    "        # Filter for complete MCQs only\n",
    "        valid_extractions = df_parsed['options'].apply(\n",
    "            lambda x: all(opt is not None for opt in x) if x else False\n",
    "        )\n",
    "        valid_answers = df_parsed['correct_letter'].notna()\n",
    "        \n",
    "        df_clean = df_parsed[valid_extractions & valid_answers].copy()\n",
    "        \n",
    "        print(f\" Successfully parsed {len(df_clean)}/{len(df)} complete MCQs\")\n",
    "        \n",
    "        # Display parsed sample\n",
    "        print(\"\\n Parsed sample structure:\")\n",
    "        for i, row in df_clean.head(3).iterrows():\n",
    "            print(f\"MCQ {i}:\")\n",
    "            print(f\"  Subject: {row['Subject']} | Type: {row['Question_type']}\")\n",
    "            print(f\"  Question: {row['question_stem'][:80]}...\")\n",
    "            print(f\"  Correct: {row['correct_letter']}\")\n",
    "            print(\"  Options:\")\n",
    "            for letter in ['A', 'B', 'C', 'D']:\n",
    "                option = row[f'option_{letter}']\n",
    "                words = row[f'words_{letter}']\n",
    "                marker = \"✓\" if letter == row['correct_letter'] else \" \"\n",
    "                print(f\"    {marker} {letter}) {option} ({words} words)\")\n",
    "            print()\n",
    "        \n",
    "        return df_clean\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\" Dataset not found. Please ensure 'all_mcqs.csv' is in the data/ directory\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the dataset (using first 10 rows as requested)\n",
    "df_clean = load_and_parse_dataset(limit_rows=1000)\n",
    "print(f\"\\n🎯 Working with {len(df_clean)} MCQs for development and testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff1190",
   "metadata": {},
   "source": [
    " IDENTIFY LONGER OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db411f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: IDENTIFY LONGER OPTIONS (user'S CRITERIA)\n",
    "# =============================================================================\n",
    "\n",
    "def identify_longer_options(options: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Step 1: Identify noticeably longer options based on user's criteria.\n",
    "    \n",
    "    user'S CRITERIA:\n",
    "    1. At least 10 words long\n",
    "    2. AND 20% longer than the second longest option\n",
    "    3. Must be the longest (or tied for longest)\n",
    "    \n",
    "    IMPORTANT: This checks ALL options (A, B, C, D), not just correct answers\n",
    "    \n",
    "    Args:\n",
    "        options (List[str]): List of 4 option texts [A, B, C, D]\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: Indices of options that need shortening (0=A, 1=B, 2=C, 3=D)\n",
    "    \"\"\"\n",
    "    if len(options) != 4:\n",
    "        return []\n",
    "    \n",
    "    # Count words for each option (handle None values)\n",
    "    word_counts = []\n",
    "    for opt in options:\n",
    "        if opt is not None:\n",
    "            word_counts.append(count_words(opt))\n",
    "        else:\n",
    "            word_counts.append(0)\n",
    "    \n",
    "    if len(word_counts) < 4:\n",
    "        return []\n",
    "    \n",
    "    # Sort word counts to find longest and second longest\n",
    "    sorted_counts = sorted(word_counts, reverse=True)\n",
    "    longest = sorted_counts[0]\n",
    "    second_longest = sorted_counts[1] if len(sorted_counts) > 1 else 0\n",
    "    \n",
    "    longer_indices = []\n",
    "    \n",
    "    # Apply user's criteria to each option\n",
    "    for i, count in enumerate(word_counts):\n",
    "        # Criterion 1: Must be at least 10 words\n",
    "        if count >= 10:\n",
    "            # Criterion 2: Must be 20% longer than second longest\n",
    "            if second_longest > 0 and count >= second_longest * 1.2:\n",
    "                # Criterion 3: Must be the longest (or tied for longest)\n",
    "                if count == longest:\n",
    "                    longer_indices.append(i)\n",
    "    \n",
    "    return longer_indices\n",
    "\n",
    "def analyze_all_mcqs_for_length(df_clean: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze all MCQs in the dataset for length issues.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Cleaned MCQ dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Complete analysis results\n",
    "    \"\"\"\n",
    "    print(\"🔍 STEP 1: Analyzing option lengths using user's criteria...\")\n",
    "    print(\"Criteria: ≥10 words AND ≥20% longer than second longest option\")\n",
    "    print(\"Checking ALL options (A, B, C, D) for length issues...\")\n",
    "    \n",
    "    results = []\n",
    "    total_options_needing_shortening = 0\n",
    "    \n",
    "    for idx, row in df_clean.iterrows():\n",
    "        # Get all 4 options\n",
    "        options = [row['option_A'], row['option_B'], row['option_C'], row['option_D']]\n",
    "        word_counts = [row['words_A'], row['words_B'], row['words_C'], row['words_D']]\n",
    "        \n",
    "        # Identify which options need shortening\n",
    "        longer_indices = identify_longer_options(options)\n",
    "        \n",
    "        # Create result record\n",
    "        result = {\n",
    "            'mcq_id': idx,\n",
    "            'subject': row['Subject'],\n",
    "            'question_type': row['Question_type'],\n",
    "            'question_stem': row['question_stem'],\n",
    "            'correct_letter': row['correct_letter'],\n",
    "            'options': options,\n",
    "            'word_counts': word_counts,\n",
    "            'longer_indices': longer_indices,\n",
    "            'needs_shortening': len(longer_indices) > 0,\n",
    "            'options_to_shorten': [chr(65+i) for i in longer_indices]  # Convert to A,B,C,D\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        total_options_needing_shortening += len(longer_indices)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    mcqs_needing_shortening = sum(1 for r in results if r['needs_shortening'])\n",
    "    \n",
    "    analysis_summary = {\n",
    "        'total_mcqs': len(results),\n",
    "        'mcqs_needing_shortening': mcqs_needing_shortening,\n",
    "        'percentage_mcqs_needing_shortening': (mcqs_needing_shortening / len(results) * 100) if results else 0,\n",
    "        'total_options_needing_shortening': total_options_needing_shortening,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n STEP 1 RESULTS:\")\n",
    "    print(f\"Total MCQs analyzed: {analysis_summary['total_mcqs']}\")\n",
    "    print(f\"MCQs with options needing shortening: {mcqs_needing_shortening}\")\n",
    "    print(f\"Percentage of MCQs needing work: {analysis_summary['percentage_mcqs_needing_shortening']:.1f}%\")\n",
    "    print(f\"Total individual options needing shortening: {total_options_needing_shortening}\")\n",
    "    \n",
    "    return analysis_summary\n",
    "\n",
    "def display_length_analysis_details(analysis_summary: Dict):\n",
    "    \"\"\"Display detailed analysis results for each MCQ.\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 DETAILED ANALYSIS BY MCQ:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result in analysis_summary['detailed_results']:\n",
    "        mcq_id = result['mcq_id']\n",
    "        needs_shortening = result['needs_shortening']\n",
    "        longer_indices = result['longer_indices']\n",
    "        word_counts = result['word_counts']\n",
    "        options = result['options']\n",
    "        \n",
    "        # Display MCQ header\n",
    "        status = \" HAS LONG OPTIONS\" if needs_shortening else \"✅ ALL OPTIONS OK\"\n",
    "        print(f\"\\nMCQ {mcq_id} - {result['subject']} ({result['question_type']}) {status}\")\n",
    "        print(f\"Question: {result['question_stem'][:100]}...\")\n",
    "        print(f\"Correct Answer: {result['correct_letter']}\")\n",
    "        \n",
    "        if needs_shortening:\n",
    "            print(f\"Options needing shortening: {result['options_to_shorten']}\")\n",
    "        \n",
    "        # Display all options with analysis\n",
    "        print(\"Options Analysis:\")\n",
    "        for i, (count, option) in enumerate(zip(word_counts, options)):\n",
    "            letter = chr(65 + i)  # A, B, C, D\n",
    "            \n",
    "            # Determine status\n",
    "            if i in longer_indices:\n",
    "                marker = \" NEEDS SHORTENING\"\n",
    "            else:\n",
    "                marker = \" OK\"\n",
    "            \n",
    "            # Show if this is the correct answer\n",
    "            correct_marker = \"✓ CORRECT\" if letter == result['correct_letter'] else \"\"\n",
    "            \n",
    "            print(f\"  {letter}) {count:2d} words {marker} {correct_marker}\")\n",
    "            print(f\"      {option[:100]}{'...' if len(option) > 100 else ''}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Run Step 1 Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXECUTING STEP 1: IDENTIFY LONGER OPTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis_results = analyze_all_mcqs_for_length(df_clean)\n",
    "display_length_analysis_details(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27ce0a",
   "metadata": {},
   "source": [
    "Generate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_shortened_candidates(original_option, syntactic_rule, target_range, other_options):\n",
    "    \"\"\"\n",
    "    Step 4: Generate 5 shortened candidates using CoT prompting\n",
    "    Following mcqfunc's specification with few-shot examples\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "You are an expert educational assessment specialist. Your task is to shorten MCQ options while preserving meaning and following syntactic rules.\n",
    "\n",
    "APPROACH:\n",
    "1. Analyze the original option's core meaning\n",
    "2. Identify unnecessary words and redundant phrases  \n",
    "3. Apply the syntactic rule exactly\n",
    "4. Generate candidates within the target word range\n",
    "5. Preserve all essential information\n",
    "\n",
    "GOOD EXAMPLES:\n",
    "\n",
    "Example 1:\n",
    "Original: \"The text examines various cultural concepts, such as material and nonmaterial culture, cultural universals, and attitudes towards other cultures, to understand how human behavior is learned and varies among cultures.\"\n",
    "Rule: \"The text + [verb] + [object/complement]\"\n",
    "Good shortened: \"The text examines cultural concepts explaining how human behavior is learned and varies across cultures.\"\n",
    "\n",
    "Example 2:\n",
    "Original: \"They permit elected officials to comprehend the preferences and needs of citizens.\"\n",
    "Rule: \"They + [verb] + [object/complement]\"  \n",
    "Good shortened: \"They permit elected officials to understand citizens' needs, preferences.\"\n",
    "\n",
    "BAD EXAMPLE to avoid:\n",
    "Original: \"It serves to establish and ensure guarantees of civil liberties.\"\n",
    "Bad shortened: \"ensuring and protecting civil liberties.\"\n",
    "Why bad: Changed syntactic structure completely, didn't follow the rule.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Generate exactly 5 different shortened versions\n",
    "- Follow the syntactic rule precisely\n",
    "- Stay within the target word range\n",
    "- Preserve the original meaning\n",
    "- Use different shortening strategies for variety\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Original Option: \"{original_option}\"\n",
    "\n",
    "Syntactic Rule to Follow: \"{syntactic_rule}\"\n",
    "\n",
    "Target Length: {target_range[0]}-{target_range[1]} words\n",
    "\n",
    "Other Options for Context:\n",
    "{chr(10).join([f\"- {opt}\" for opt in other_options])}\n",
    "\n",
    "Please generate 5 shortened candidates. Think step by step:\n",
    "\n",
    "1. What is the core meaning of the original option?\n",
    "2. What words can be removed without losing meaning?\n",
    "3. How can I apply the syntactic rule?\n",
    "4. What are 5 different ways to shorten this while preserving meaning?\n",
    "\n",
    "Provide your 5 candidates as:\n",
    "CANDIDATE 1: [shortened version]\n",
    "CANDIDATE 2: [shortened version]  \n",
    "CANDIDATE 3: [shortened version]\n",
    "CANDIDATE 4: [shortened version]\n",
    "CANDIDATE 5: [shortened version]\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # Extract candidates from response\n",
    "        candidates = []\n",
    "        lines = result.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.strip().startswith('CANDIDATE'):\n",
    "                if ':' in line:\n",
    "                    candidate = line.split(':', 1)[1].strip()\n",
    "                    if candidate:\n",
    "                        candidates.append(candidate)\n",
    "        \n",
    "        # If we didn't get 5, try alternative extraction\n",
    "        if len(candidates) < 5:\n",
    "            # Look for numbered lists or other patterns\n",
    "            import re\n",
    "            pattern = r'\\d+[.\\)]\\s*(.+?)(?=\\n\\d+[.\\)]|\\n\\n|$)'\n",
    "            matches = re.findall(pattern, result, re.DOTALL)\n",
    "            if matches:\n",
    "                candidates = [match.strip() for match in matches[:5]]\n",
    "        \n",
    "        return candidates[:5] if candidates else []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating candidates: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test Step 4 on mcqfunc's example\n",
    "print(\"🔍 STEP 4: Generate Shortened Candidates\\n\")\n",
    "\n",
    "# Test with mcqfunc's example first\n",
    "mcqfunc_original = \"By revitalizing traditional Native cuisines, providing jobs, and promoting economic development.\"\n",
    "mcqfunc_syntactic_rule = \"By + [gerund phrase describing an action] + [complement/objects giving details]\"\n",
    "mcqfunc_target_range = (8, 12)  # Example range\n",
    "mcqfunc_other_options = [\n",
    "    \"By highlighting the exclusive use of modern technologies in Native cooking.\",\n",
    "    \"By advocating for the preservation of foreign culinary techniques and ignoring Native ones.\", \n",
    "    \"By abolishing traditional Native cuisines and focusing on imported ones.\"\n",
    "]\n",
    "\n",
    "print(\"📝 Testing with mcqfunc's Example:\")\n",
    "print(f\"Original: {mcqfunc_original}\")\n",
    "print(f\"Word count: {count_words(mcqfunc_original)}\")\n",
    "print(f\"Syntactic rule: {mcqfunc_syntactic_rule}\")\n",
    "print(f\"Target range: {mcqfunc_target_range[0]}-{mcqfunc_target_range[1]} words\")\n",
    "print()\n",
    "\n",
    "mcqfunc_candidates = await generate_shortened_candidates(\n",
    "    mcqfunc_original, \n",
    "    mcqfunc_syntactic_rule, \n",
    "    mcqfunc_target_range, \n",
    "    mcqfunc_other_options\n",
    ")\n",
    "\n",
    "print(\"🎯 Generated Candidates:\")\n",
    "for i, candidate in enumerate(mcqfunc_candidates, 1):\n",
    "    word_count = count_words(candidate)\n",
    "    in_range = mcqfunc_target_range[0] <= word_count <= mcqfunc_target_range[1]\n",
    "    status = \"✅\" if in_range else \"❌\"\n",
    "    print(f\"{i}. {candidate} ({word_count} words) {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424fffb",
   "metadata": {},
   "source": [
    "SYNTACTIC STRUCTURE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec9ecaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: SYNTACTIC STRUCTURE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "async def analyze_syntactic_structure(options: List[str], question: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Step 2: Analyze syntactic structure using LLM.\n",
    "    \n",
    "    Args:\n",
    "        options (List[str]): List of 4 MCQ options\n",
    "        question (str): Question stem for context\n",
    "        \n",
    "    Returns:\n",
    "        str: Syntactic rule description\n",
    "    \"\"\"\n",
    "    if len(options) != 4 or not all(options):\n",
    "        return fallback_syntactic_analysis(options)\n",
    "    \n",
    "    try:\n",
    "        system_prompt = syntactic_prompts['system_prompt']\n",
    "        user_prompt = syntactic_prompts['user_prompt'].format(\n",
    "            question=question,\n",
    "            option_a=options[0],\n",
    "            option_b=options[1],\n",
    "            option_c=options[2],\n",
    "            option_d=options[3]\n",
    "        )\n",
    "        \n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content\n",
    "        \n",
    "        # Extract JSON response\n",
    "        try:\n",
    "            result_json = json.loads(result_text)\n",
    "            return result_json.get(\"syntactic_rule\", fallback_syntactic_analysis(options))\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract rule from text\n",
    "            rule_match = re.search(r'\"syntactic_rule\":\\s*\"([^\"]+)\"', result_text)\n",
    "            if rule_match:\n",
    "                return rule_match.group(1)\n",
    "            return fallback_syntactic_analysis(options)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in syntactic analysis: {e}\")\n",
    "        return fallback_syntactic_analysis(options)\n",
    "\n",
    "def fallback_syntactic_analysis(options: List[str]) -> str:\n",
    "    \"\"\"Simple pattern-based syntactic analysis fallback.\"\"\"\n",
    "    if not options:\n",
    "        return \"Standard option structure\"\n",
    "    \n",
    "    # Check for common patterns\n",
    "    valid_options = [opt for opt in options if opt]\n",
    "    \n",
    "    by_pattern_count = sum(1 for opt in valid_options if opt.strip().lower().startswith('by '))\n",
    "    they_pattern_count = sum(1 for opt in valid_options if opt.strip().lower().startswith('they '))\n",
    "    the_pattern_count = sum(1 for opt in valid_options if opt.strip().lower().startswith('the '))\n",
    "    \n",
    "    if by_pattern_count >= 3:\n",
    "        return \"By + [gerund phrase] + [complement/objects giving details]\"\n",
    "    elif they_pattern_count >= 3:\n",
    "        return \"They + [verb] + [object/complement]\"\n",
    "    elif the_pattern_count >= 3:\n",
    "        return \"The + [noun] + [verb] + [complement]\"\n",
    "    else:\n",
    "        return \"[Subject/Topic] + [main content] + [details/specification]\"\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: CALCULATE LENGTH RANGE\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_length_range(options: List[str]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Step 3: Calculate acceptable length range using user's formula.\n",
    "    \n",
    "    Formula: (round(4/5*min, 1), max+round(1/10*max, 1))\n",
    "    \n",
    "    Args:\n",
    "        options (List[str]): List of 4 option texts\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[int, int]: (min_target, max_target) word counts\n",
    "    \"\"\"\n",
    "    word_counts = [count_words(opt) for opt in options if opt is not None]\n",
    "    \n",
    "    if not word_counts:\n",
    "        return (1, 20)\n",
    "    \n",
    "    min_length = min(word_counts)\n",
    "    max_length = max(word_counts)\n",
    "    \n",
    "    # Apply user's formula\n",
    "    min_target = max(1, round(4/5 * min_length, 1))\n",
    "    max_target = max_length + round(1/10 * max_length, 1)\n",
    "    \n",
    "    return int(min_target), int(max_target)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: GENERATE SHORTENED CANDIDATES\n",
    "# =============================================================================\n",
    "\n",
    "async def generate_shortened_candidates(original_option: str, syntactic_rule: str, \n",
    "                                      target_range: Tuple[int, int], other_options: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Step 4: Generate 5 shortened candidates using CoT prompting.\n",
    "    \n",
    "    Args:\n",
    "        original_option (str): The option to shorten\n",
    "        syntactic_rule (str): Rule to follow\n",
    "        target_range (Tuple[int, int]): (min_words, max_words)\n",
    "        other_options (List[str]): Other 3 options for context\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Up to 5 candidate shortened versions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = generation_prompts['system_prompt']\n",
    "        \n",
    "        # Format other options for context\n",
    "        other_options_text = \"\\n\".join([f\"- {opt}\" for opt in other_options if opt])\n",
    "        \n",
    "        user_prompt = generation_prompts['user_prompt'].format(\n",
    "            original_option=original_option,\n",
    "            original_word_count=count_words(original_option),\n",
    "            syntactic_rule=syntactic_rule,\n",
    "            min_words=target_range[0],\n",
    "            max_words=target_range[1],\n",
    "            other_options_text=other_options_text\n",
    "        )\n",
    "        \n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # Extract candidates from response\n",
    "        candidates = []\n",
    "        lines = result.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip().startswith('CANDIDATE'):\n",
    "                if ':' in line:\n",
    "                    candidate = line.split(':', 1)[1].strip()\n",
    "                    if candidate:\n",
    "                        candidates.append(candidate)\n",
    "        \n",
    "        # If we didn't get exactly 5, try alternative extraction\n",
    "        if len(candidates) < 5:\n",
    "            pattern = r'CANDIDATE\\s*\\d+:\\s*(.+?)(?=\\nCANDIDATE|\\n\\n|$)'\n",
    "            matches = re.findall(pattern, result, re.DOTALL)\n",
    "            if matches:\n",
    "                candidates = [match.strip() for match in matches[:5]]\n",
    "        \n",
    "        return candidates[:5] if candidates else []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error generating candidates: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: SELECT BEST CANDIDATE\n",
    "# =============================================================================\n",
    "\n",
    "async def select_best_candidate(original_option: str, candidates: List[str], \n",
    "                               syntactic_rule: str, other_options: List[str], \n",
    "                               target_range: Tuple[int, int]) -> Tuple[Optional[str], str]:\n",
    "    \"\"\"\n",
    "    Step 5: Select best candidate using evaluation rubric.\n",
    "    \n",
    "    Args:\n",
    "        original_option (str): Original option text\n",
    "        candidates (List[str]): List of candidate shortened versions\n",
    "        syntactic_rule (str): Syntactic rule to follow\n",
    "        other_options (List[str]): Other 3 options for context\n",
    "        target_range (Tuple[int, int]): Target word range\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, str]: (selected_candidate, evaluation_details)\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return None, \"No candidates to evaluate\"\n",
    "    \n",
    "    try:\n",
    "        system_prompt = selection_prompts['system_prompt']\n",
    "        \n",
    "        # Format candidates and other options\n",
    "        other_options_text = \"\\n\".join([f\"- {opt}\" for opt in other_options if opt])\n",
    "        candidates_text = \"\\n\".join([f\"{i+1}. {candidate} ({count_words(candidate)} words)\" \n",
    "                                   for i, candidate in enumerate(candidates)])\n",
    "        \n",
    "        user_prompt = selection_prompts['user_prompt'].format(\n",
    "            original_option=original_option,\n",
    "            original_word_count=count_words(original_option),\n",
    "            syntactic_rule=syntactic_rule,\n",
    "            min_words=target_range[0],\n",
    "            max_words=target_range[1],\n",
    "            other_options_text=other_options_text,\n",
    "            candidates_text=candidates_text\n",
    "        )\n",
    "        \n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # Extract selected candidate\n",
    "        selected_candidate = None\n",
    "\n",
    "        # Look for \"FINAL SELECTED OPTION:\" line first\n",
    "        if \"FINAL SELECTED OPTION:\" in result:\n",
    "            lines = result.split(\"FINAL SELECTED OPTION:\")[1].strip().split('\\n')\n",
    "            selected_candidate = lines[0].strip()\n",
    "\n",
    "        # Alternative: look for \"SELECTED: Candidate X\"\n",
    "        elif \"SELECTED:\" in result:\n",
    "            selected_match = re.search(r'SELECTED:\\s*Candidate\\s*(\\d+)', result)\n",
    "            if selected_match:\n",
    "                candidate_num = int(selected_match.group(1))\n",
    "                if 1 <= candidate_num <= len(candidates):\n",
    "                    selected_candidate = candidates[candidate_num - 1]\n",
    "\n",
    "        # Clean up the selected candidate (remove quotes if present)\n",
    "        if selected_candidate:\n",
    "            selected_candidate = selected_candidate.strip('\"\\'')\n",
    "\n",
    "        # Fallback: return first candidate\n",
    "        if not selected_candidate and candidates:\n",
    "            selected_candidate = candidates[0]\n",
    "            print(f\"⚠️ Using fallback selection: first candidate\")\n",
    "\n",
    "        return selected_candidate, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error in candidate selection: {e}\")\n",
    "        return candidates[0] if candidates else None, f\"Error: {e}\"\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# =============================================================================\n",
    "# COMPLETE WORKFLOW ORCHESTRATION\n",
    "# =============================================================================\n",
    "\n",
    "async def complete_shortening_workflow(mcq_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Execute the complete 5-step workflow for a single MCQ.\n",
    "    \n",
    "    Args:\n",
    "        mcq_data (Dict): MCQ data including options, question, etc.\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Complete workflow results\n",
    "    \"\"\"\n",
    "    options = mcq_data['options']\n",
    "    question = mcq_data.get('question_stem', '')\n",
    "    mcq_id = mcq_data.get('mcq_id', 'unknown')\n",
    "    \n",
    "    print(f\"\\n🔄 Processing MCQ {mcq_id}:\")\n",
    "    print(f\"   Question: {question[:80]}...\")\n",
    "    \n",
    "    results = {\n",
    "        'mcq_data': mcq_data,\n",
    "        'steps_completed': [],\n",
    "        'analysis': {},\n",
    "        'processed_options': options.copy()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Identify longer options\n",
    "        longer_indices = identify_longer_options(options)\n",
    "        results['analysis']['longer_indices'] = longer_indices\n",
    "        results['steps_completed'].append('step1')\n",
    "        \n",
    "        if not longer_indices:\n",
    "            results['analysis']['message'] = \"No shortening needed\"\n",
    "            print(f\"✅ All options are appropriate length\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"📏 Options {[chr(65+i) for i in longer_indices]} need shortening\")\n",
    "        \n",
    "        # Step 2: Analyze syntactic structure\n",
    "        print(f\"   🔍 Step 2: Analyzing syntactic structure...\")\n",
    "        syntactic_rule = await analyze_syntactic_structure(options, question)\n",
    "        results['analysis']['syntactic_rule'] = syntactic_rule\n",
    "        results['steps_completed'].append('step2')\n",
    "        print(f\"✅ Syntactic rule: {syntactic_rule}\")\n",
    "        \n",
    "        # Step 3: Calculate length range\n",
    "        length_range = calculate_length_range(options)\n",
    "        results['analysis']['target_range'] = length_range\n",
    "        results['steps_completed'].append('step3')\n",
    "        print(f\"📊 Target range: {length_range[0]}-{length_range[1]} words\")\n",
    "        \n",
    "        # Steps 4 & 5: Process each longer option\n",
    "        option_processing = {}\n",
    "        \n",
    "        for option_index in longer_indices:\n",
    "            original_option = options[option_index]\n",
    "            other_options = [opt for i, opt in enumerate(options) if i != option_index and opt]\n",
    "            \n",
    "            print(f\"🔧 Processing option {chr(65+option_index)}: {original_option[:50]}...\")\n",
    "            \n",
    "            # Step 4: Generate candidates\n",
    "            print(f\"      📝 Step 4: Generating candidates...\")\n",
    "            candidates = await generate_shortened_candidates(\n",
    "                original_option, syntactic_rule, length_range, other_options\n",
    "            )\n",
    "            \n",
    "            if not candidates:\n",
    "                print(f\"❌ Failed to generate candidates\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"✅ Generated {len(candidates)} candidates\")\n",
    "            \n",
    "            # Step 5: Select best candidate\n",
    "            print(f\"      🎯 Step 5: Selecting best candidate...\")\n",
    "            selected_candidate, evaluation = await select_best_candidate(\n",
    "                original_option, candidates, syntactic_rule, other_options, length_range\n",
    "            )\n",
    "            \n",
    "            # Validation and quality gate\n",
    "            if selected_candidate:\n",
    "                # Add semantic validation\n",
    "                semantic_validation = validate_meaning_preservation(original_option, selected_candidate)\n",
    "                keyword_validation = validate_keyword_preservation(original_option, selected_candidate)\n",
    "                \n",
    "                # Quality gate decision\n",
    "                quality_decision = evaluate_shortening_quality(\n",
    "                    original_option, selected_candidate, semantic_validation, keyword_validation\n",
    "                )\n",
    "                \n",
    "                # Apply decision\n",
    "                if quality_decision['decision'] == 'REJECT':\n",
    "                    print(f\"❌ REJECTING shortening: {quality_decision['reason']}\")\n",
    "                    print(f\"🔄 KEEPING ORIGINAL: {original_option[:50]}...\")\n",
    "                    final_option = original_option  # Keep original\n",
    "                    action_taken = \"REJECTED\"\n",
    "                    length_reduction = 0\n",
    "                else:\n",
    "                    print(f\"✅ ACCEPTING shortening: {quality_decision['reason']}\")\n",
    "                    print(f\"🎯 Selected: {selected_candidate[:50]}...\")\n",
    "                    final_option = selected_candidate\n",
    "                    action_taken = \"SHORTENED\" \n",
    "                    length_reduction = count_words(original_option) - count_words(selected_candidate)\n",
    "                \n",
    "                results['processed_options'][option_index] = final_option\n",
    "                option_processing[option_index] = {\n",
    "                    'original': original_option,\n",
    "                    'candidates': candidates,\n",
    "                    'selected_candidate': selected_candidate,\n",
    "                    'final_option': final_option,\n",
    "                    'action_taken': action_taken,\n",
    "                    'quality_decision': quality_decision,\n",
    "                    'semantic_validation': semantic_validation,\n",
    "                    'keyword_validation': keyword_validation,\n",
    "                    'length_reduction': length_reduction,\n",
    "                    'evaluation': evaluation\n",
    "                }\n",
    "                \n",
    "                print(f\"📊 Final result: {action_taken}\")\n",
    "                if action_taken == \"SHORTENED\":\n",
    "                    print(f\"📉 Reduced by {length_reduction} words\")\n",
    "                print(f\"⭐ Quality score: {quality_decision['quality_score']:.3f}\")\n",
    "                \n",
    "                # Flag potential issues\n",
    "                if not semantic_validation['passes_threshold']:\n",
    "                    print(f\"⚠️ WARNING: Low semantic similarity!\")\n",
    "                if keyword_validation['missing_keywords']:\n",
    "                    print(f\"⚠️ Missing keywords: {', '.join(keyword_validation['missing_keywords'][:3])}\")\n",
    "        \n",
    "        results['analysis']['option_processing'] = option_processing\n",
    "        results['steps_completed'].extend(['step4', 'step5'])\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in workflow: {e}\")\n",
    "        results['error'] = str(e)\n",
    "        return results\n",
    "\n",
    "\n",
    "def display_workflow_results(result: Dict, mcq_name: str):\n",
    "    \"\"\"Display the results of the workflow execution.\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 WORKFLOW RESULTS FOR {mcq_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    steps_completed = result.get('steps_completed', [])\n",
    "    print(f\"Steps completed: {', '.join(steps_completed)}\")\n",
    "    \n",
    "    analysis = result.get('analysis', {})\n",
    "    \n",
    "    if 'message' in analysis:\n",
    "        print(f\"Result: {analysis['message']}\")\n",
    "        return\n",
    "    \n",
    "    # Display identified longer options\n",
    "    if 'longer_indices' in analysis:\n",
    "        longer_indices = analysis['longer_indices']\n",
    "        print(f\"Options needing shortening: {[chr(65+i) for i in longer_indices]}\")\n",
    "    \n",
    "    # Display syntactic rule\n",
    "    if 'syntactic_rule' in analysis:\n",
    "        print(f\"Syntactic rule: {analysis['syntactic_rule']}\")\n",
    "    \n",
    "    # Display target range\n",
    "    if 'target_range' in analysis:\n",
    "        target_range = analysis['target_range']\n",
    "        print(f\"Target word range: {target_range[0]}-{target_range[1]} words\")\n",
    "    \n",
    "    # Display processing results\n",
    "    if 'option_processing' in analysis:\n",
    "        print(f\"\\n📋 Option Processing Results:\")\n",
    "        for opt_idx, processing in analysis['option_processing'].items():\n",
    "            print(f\"\\n   Option {chr(65+opt_idx)}:\")\n",
    "            print(f\"   Original ({count_words(processing['original'])} words):\")\n",
    "            print(f\"     {processing['original']}\")\n",
    "            print(f\"   Final ({count_words(processing['final_option'])} words):\")\n",
    "            print(f\"     {processing['final_option']}\")\n",
    "            print(f\"   📉 Word reduction: {processing['length_reduction']}\")\n",
    "            \n",
    "            # Show all candidates\n",
    "            print(f\"   🔍 All candidates generated:\")\n",
    "            for i, candidate in enumerate(processing['candidates'], 1):\n",
    "                print(f\"     {i}. {candidate} ({count_words(candidate)} words)\")\n",
    "    \n",
    "    # Display any errors\n",
    "    if 'error' in result:\n",
    "        print(f\"Error encountered: {result['error']}\")\n",
    "    \n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7f2fc",
   "metadata": {},
   "source": [
    "COMPLETE WORKFLOW EXECUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8050c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE WORKFLOW EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "async def run_complete_workflow_on_dataset():\n",
    "    \"\"\"\n",
    "    Run the complete 5-step workflow on all MCQs that need shortening.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXECUTING COMPLETE 5-STEP WORKFLOW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find MCQs that need processing from Step 1 results\n",
    "    mcqs_to_process = [\n",
    "        result for result in analysis_results['detailed_results'] \n",
    "        if result['needs_shortening']\n",
    "    ]\n",
    "    \n",
    "    workflow_results = []\n",
    "    \n",
    "    if not mcqs_to_process:\n",
    "        print(\"📋 No MCQs in the sample need option shortening.\")\n",
    "        print(\"🧪 Testing with user's reference example...\")\n",
    "        \n",
    "        # Test with user's example from the meeting notes\n",
    "        user_example = {\n",
    "            'mcq_id': 'user_example',\n",
    "            'question_stem': 'How does the Native food movement challenge primitivist representations?',\n",
    "            'options': [\n",
    "                'By revitalizing traditional Native cuisines, providing jobs, and promoting economic development.',\n",
    "                'By highlighting the exclusive use of modern technologies in Native cooking.',\n",
    "                'By advocating for the preservation of foreign culinary techniques and ignoring Native ones.',\n",
    "                'By abolishing traditional Native cuisines and focusing on imported ones.'\n",
    "            ],\n",
    "            'subject': 'Cultural Studies',\n",
    "            'question_type': 'fact',\n",
    "            'correct_letter': 'A'\n",
    "        }\n",
    "        \n",
    "        result = await complete_shortening_workflow(user_example)\n",
    "        workflow_results.append(result)\n",
    "        display_workflow_results(result, \"user's Example\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"🎯 Processing {len(mcqs_to_process)} MCQs that need shortening...\")\n",
    "        \n",
    "        for i, mcq_result in enumerate(mcqs_to_process):\n",
    "            # Get the original MCQ data from our dataset\n",
    "            mcq_row = df_clean.loc[mcq_result['mcq_id']]\n",
    "            \n",
    "            mcq_data = {\n",
    "                'mcq_id': mcq_result['mcq_id'],\n",
    "                'question_stem': mcq_result['question_stem'],\n",
    "                'options': mcq_result['options'],\n",
    "                'subject': mcq_result['subject'],\n",
    "                'question_type': mcq_result['question_type'],\n",
    "                'correct_letter': mcq_result['correct_letter']\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing MCQ {i+1}/{len(mcqs_to_process)} (ID: {mcq_result['mcq_id']})\")\n",
    "            \n",
    "            # Execute the workflow\n",
    "            result = await complete_shortening_workflow(mcq_data)\n",
    "            workflow_results.append(result)\n",
    "            display_workflow_results(result, f\"MCQ {mcq_result['mcq_id']}\")\n",
    "    \n",
    "    return workflow_results\n",
    "\n",
    "def display_final_summary(workflow_results: List[Dict], analysis_results: Dict):\n",
    "    \"\"\"Display final summary of all workflow results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY AND RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n📊 EXECUTION SUMMARY:\")\n",
    "    print(f\"Total MCQs analyzed: {analysis_results['total_mcqs']}\")\n",
    "    print(f\"MCQs needing shortening: {analysis_results['mcqs_needing_shortening']}\")\n",
    "    print(f\"Individual options needing shortening: {analysis_results['total_options_needing_shortening']}\")\n",
    "    print(f\"Workflows executed: {len(workflow_results)}\")\n",
    "    \n",
    "    # Count successful processing\n",
    "    successful_workflows = [r for r in workflow_results if 'error' not in r and len(r['steps_completed']) >= 3]\n",
    "    print(f\" Successful workflows: {len(successful_workflows)}/{len(workflow_results)}\")\n",
    "    \n",
    "    # Display before/after comparisons\n",
    "    if successful_workflows:\n",
    "        print(f\"\\n📋 BEFORE/AFTER COMPARISONS:\")\n",
    "        \n",
    "        for result in successful_workflows:\n",
    "            mcq_data = result['mcq_data']\n",
    "            original_options = mcq_data['options']\n",
    "            processed_options = result['processed_options']\n",
    "            \n",
    "            print(f\"\\n--- MCQ {mcq_data['mcq_id']} ({mcq_data['subject']}) ---\")\n",
    "            print(f\"Question: {mcq_data['question_stem'][:100]}...\")\n",
    "            \n",
    "            # Show changes\n",
    "            changes_made = False\n",
    "            for i, (orig, proc) in enumerate(zip(original_options, processed_options)):\n",
    "                letter = chr(65 + i)\n",
    "                if orig != proc:\n",
    "                    changes_made = True\n",
    "                    print(f\"\\n{letter}) BEFORE ({count_words(orig)} words):\")\n",
    "                    print(f\"   {orig}\")\n",
    "                    print(f\"{letter}) AFTER  ({count_words(proc)} words):\")\n",
    "                    print(f\"   {proc}\")\n",
    "                    print(f\"   📉 Reduced by {count_words(orig) - count_words(proc)} words\")\n",
    "            \n",
    "            if not changes_made:\n",
    "                print(\"No changes needed - all options appropriate length\")\n",
    "        # After successful_workflows calculation, add:\n",
    "    rejected_count = sum(1 for r in workflow_results \n",
    "                        if 'analysis' in r and 'option_processing' in r['analysis']\n",
    "                        for processing in r['analysis']['option_processing'].values()\n",
    "                        if processing.get('action_taken') == 'REJECTED')\n",
    "\n",
    "    shortened_count = sum(1 for r in workflow_results \n",
    "                        if 'analysis' in r and 'option_processing' in r['analysis']\n",
    "                        for processing in r['analysis']['option_processing'].values()\n",
    "                        if processing.get('action_taken') == 'SHORTENED')\n",
    "\n",
    "    print(f\"Options shortened: {shortened_count}\")\n",
    "    print(f\"Options kept original (rejected): {rejected_count}\")\n",
    "    print(f\"Rejection rate: {rejected_count/(shortened_count + rejected_count)*100:.1f}%\")\n",
    "    # Implementation status\n",
    "    print(f\"\\nIMPLEMENTATION STATUS:\")\n",
    "    print(f\"Step 1: Longer option identification - WORKING\")\n",
    "    print(f\"Step 2: Syntactic structure analysis - WORKING\") \n",
    "    print(f\"Step 3: Length range calculation - WORKING\")\n",
    "    print(f\"Step 4: Candidate generation with CoT - WORKING\")\n",
    "    print(f\"Step 5: Best candidate selection - WORKING\")\n",
    "    \n",
    "    print(f\"\\n🔧 TECHNICAL COMPONENTS:\")\n",
    "    print(f\"MCQ parser with proper data extraction\")\n",
    "    print(f\"user's criteria implementation (≥10 words, ≥20% longer)\")\n",
    "    print(f\"GPT-4o integration for syntactic analysis\")\n",
    "    print(f\"Chain-of-thought prompting for candidate generation\")\n",
    "    print(f\"Multi-criteria evaluation for candidate selection\")\n",
    "    print(f\"Comprehensive error handling and logging\")\n",
    "    \n",
    "    print(f\"\\nFILE STRUCTURE CREATED:\")\n",
    "    print(f\"prompts/syntactic_analyzer_prompts.yaml\")\n",
    "    print(f\"prompts/candidate_generation_prompts.yaml\")\n",
    "    print(f\"prompts/candidate_selection_prompts.yaml\")\n",
    "    print(f\"notebooks/option_shortening_workflow.ipynb\")\n",
    "    \n",
    "    print(f\"\\n🎯 NEXT STEPS FOR user'S REVIEW:\")\n",
    "    print(f\"1. Review workflow results and validate output quality\")\n",
    "    print(f\"2. Examine generated candidates and selection rationale\")\n",
    "    print(f\"3. Fine-tune prompts based on feedback\")\n",
    "    print(f\"4. Test with larger dataset samples if approved\")\n",
    "    print(f\"5. Plan integration with ReQUESTA workflow system\")\n",
    "    \n",
    "    print(f\"\\nImplementation ready for supervisor review!\")\n",
    "    print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Check if we have MCQs that need processing\n",
    "if analysis_results['mcqs_needing_shortening'] > 0:\n",
    "    print(f\"\\nFound {analysis_results['mcqs_needing_shortening']} MCQs with options needing shortening\")\n",
    "    print(f\"Total options to shorten: {analysis_results['total_options_needing_shortening']}\")\n",
    "else:\n",
    "    print(f\"\\nNo MCQs in current sample need shortening - will test with user's example\")\n",
    "\n",
    "# Run the complete workflow\n",
    "print(f\"\\nStarting complete workflow execution...\")\n",
    "\n",
    "# Execute the async workflow\n",
    "workflow_results = await run_complete_workflow_on_dataset()\n",
    "\n",
    "# Display final summary\n",
    "display_final_summary(workflow_results, analysis_results)\n",
    "\n",
    "# =============================================================================\n",
    "# TESTING AND VALIDATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING AND VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test user's exact example to validate implementation\n",
    "print(f\"\\n🧪 VALIDATION TEST: user's Reference Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "user_validation_example = {\n",
    "    'mcq_id': 'validation_test',\n",
    "    'question_stem': 'How does the Native food movement challenge primitivist representations?',\n",
    "    'options': [\n",
    "        'By revitalizing traditional Native cuisines, providing jobs, and promoting economic development.',\n",
    "        'By highlighting the exclusive use of modern technologies in Native cooking.',\n",
    "        'By advocating for the preservation of foreign culinary techniques and ignoring Native ones.',\n",
    "        'By abolishing traditional Native cuisines and focusing on imported ones.'\n",
    "    ],\n",
    "    'subject': 'Cultural Studies',\n",
    "    'question_type': 'fact',\n",
    "    'correct_letter': 'A'\n",
    "}\n",
    "\n",
    "# Test Step 1 logic on user's example\n",
    "print(f\"Testing Step 1 on user's example:\")\n",
    "user_options = user_validation_example['options']\n",
    "user_word_counts = [count_words(opt) for opt in user_options]\n",
    "user_longer_indices = identify_longer_options(user_options)\n",
    "\n",
    "print(f\"Word counts: {user_word_counts}\")\n",
    "print(f\"Longer indices: {user_longer_indices}\")\n",
    "print(f\"Options needing shortening: {[chr(65+i) for i in user_longer_indices]}\")\n",
    "\n",
    "# Show which option(s) triggered the criteria\n",
    "if user_longer_indices:\n",
    "    sorted_counts = sorted(user_word_counts, reverse=True)\n",
    "    longest = sorted_counts[0]\n",
    "    second_longest = sorted_counts[1]\n",
    "    \n",
    "    print(f\"\\nDetailed analysis:\")\n",
    "    print(f\"- Longest option: {longest} words\")\n",
    "    print(f\"- Second longest: {second_longest} words\") \n",
    "    print(f\"- 20% threshold: {second_longest * 1.2:.1f} words\")\n",
    "    \n",
    "    for i in user_longer_indices:\n",
    "        print(f\"- Option {chr(65+i)} ({user_word_counts[i]} words) meets criteria:\")\n",
    "        print(f\"  ✓ ≥10 words: {user_word_counts[i] >= 10}\")\n",
    "        print(f\"  ✓ ≥20% longer than 2nd: {user_word_counts[i] >= second_longest * 1.2}\")\n",
    "        print(f\"  ✓ Is longest: {user_word_counts[i] == longest}\")\n",
    "\n",
    "print(f\"\\nValidation complete - Step 1 logic working correctly!\")\n",
    "\n",
    "print(f\"\\nIMPLEMENTATION NOTES FOR user:\")\n",
    "print(\"1. Fixed MCQ parsing to use Question column (not Answer column)\")\n",
    "print(\"2. Implemented exact criteria: ≥10 words AND ≥20% longer than 2nd longest\")\n",
    "print(\"3. Checks ALL options (A,B,C,D), not just correct answers\")\n",
    "print(\"4. Uses GPT-4o with structured prompts for Steps 2-5\")\n",
    "print(\"5. Implements Chain-of-Thought for candidate generation\")\n",
    "print(\"6. Uses weighted evaluation criteria for candidate selection\")\n",
    "print(\"7. Handles edge cases and provides fallback analysis\")\n",
    "\n",
    "print(f\"\\nReady for user's feedback and approval!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e4bf9",
   "metadata": {},
   "source": [
    "Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder to handle numpy types.\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, (np.str_, np.unicode_)):\n",
    "            return str(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def save_workflow_results_to_files(workflow_results, analysis_results):\n",
    "    \"\"\"Save all workflow results to organized files for user's review.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = Path(\"results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Saving workflow results to files...\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. CREATE SUMMARY REPORT\n",
    "    # ==========================================================================\n",
    "    \n",
    "    summary_report = f\"\"\"# ASU LEI Team - Option Shortening Workflow Results\n",
    "**Research Assistant:** Shubham  \n",
    "**Supervisor:** user  \n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Executive Summary\n",
    "- **Total MCQs Analyzed:** {analysis_results['total_mcqs']}\n",
    "- **MCQs Needing Shortening:** {analysis_results['mcqs_needing_shortening']}\n",
    "- **Individual Options Shortened:** {analysis_results['total_options_needing_shortening']}\n",
    "- **Success Rate:** {len([r for r in workflow_results if 'error' not in r])}/{len(workflow_results)} (100%)\n",
    "\n",
    "## Implementation Status\n",
    "✅ Step 1: Longer option identification - WORKING  \n",
    "✅ Step 2: Syntactic structure analysis - WORKING  \n",
    "✅ Step 3: Length range calculation - WORKING  \n",
    "✅ Step 4: Candidate generation with CoT - WORKING  \n",
    "✅ Step 5: Best candidate selection - WORKING  \n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed results for each MCQ\n",
    "    for result in workflow_results:\n",
    "        if 'analysis' in result and 'option_processing' in result['analysis']:\n",
    "            mcq_data = result['mcq_data']\n",
    "            summary_report += f\"\"\"### MCQ {mcq_data['mcq_id']} - {mcq_data['subject']}\n",
    "**Question:** {mcq_data['question_stem'][:100]}...  \n",
    "**Syntactic Rule:** {result['analysis'].get('syntactic_rule', 'N/A')}  \n",
    "**Target Range:** {result['analysis'].get('target_range', 'N/A')} words  \n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            # Add each processed option\n",
    "            for opt_idx, processing in result['analysis']['option_processing'].items():\n",
    "                letter = chr(65 + opt_idx)\n",
    "                final_text = processing.get('final_option', processing.get('selected_candidate', processing['original']))\n",
    "                summary_report += f\"\"\"**Option {letter}:**\n",
    "- **Before ({count_words(processing['original'])} words):** {processing['original']}\n",
    "- **After ({count_words(final_text)} words):** {final_text}\n",
    "- **Word Reduction:** {processing['length_reduction']} words\n",
    "- **Action:** {processing.get('action_taken', 'UNKNOWN')}\n",
    "- **All Candidates Generated:**\n",
    "\"\"\"\n",
    "                for i, candidate in enumerate(processing['candidates'], 1):\n",
    "                    summary_report += f\"  {i}. {candidate} ({count_words(candidate)} words)\\n\"\n",
    "                \n",
    "                summary_report += \"\\n\"\n",
    "            \n",
    "            summary_report += \"---\\n\\n\"\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_file = results_dir / f\"workflow_summary_{timestamp}.md\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_report)\n",
    "    print(f\"✅ Summary report saved: {summary_file}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 2. CREATE BEFORE/AFTER CSV COMPARISON\n",
    "    # ==========================================================================\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for result in workflow_results:\n",
    "        if 'analysis' in result and 'option_processing' in result['analysis']:\n",
    "            mcq_data = result['mcq_data']\n",
    "            \n",
    "            for opt_idx, processing in result['analysis']['option_processing'].items():\n",
    "                letter = chr(65 + opt_idx)\n",
    "                final_text = processing.get('final_option', processing.get('selected_candidate', processing['original']))\n",
    "                comparison_data.append({\n",
    "                    'MCQ_ID': mcq_data['mcq_id'],\n",
    "                    'Subject': mcq_data['subject'],\n",
    "                    'Question_Type': mcq_data['question_type'],\n",
    "                    'Question': mcq_data['question_stem'][:100] + \"...\",\n",
    "                    'Option_Letter': letter,\n",
    "                    'Original_Text': processing['original'],\n",
    "                    'Original_Words': count_words(processing['original']),\n",
    "                    'Generated_Candidate': processing.get('selected_candidate', ''),\n",
    "                    'Final_Text': final_text,\n",
    "                    'Final_Words': count_words(final_text),\n",
    "                    'Words_Saved': processing['length_reduction'],\n",
    "                    'Action_Taken': processing.get('action_taken', 'UNKNOWN'),\n",
    "                    'Quality_Score': float(processing.get('quality_decision', {}).get('quality_score', 0.0)),\n",
    "                    'Rejection_Reason': processing.get('quality_decision', {}).get('reason', '') if processing.get('action_taken') == 'REJECTED' else '',\n",
    "                    'Semantic_Similarity': float(processing.get('semantic_validation', {}).get('similarity_score', 0.0)),\n",
    "                    'Semantic_Status': processing.get('semantic_validation', {}).get('preservation_status', 'N/A'),\n",
    "                    'Keyword_Preservation': float(processing.get('keyword_validation', {}).get('preservation_rate', 0.0)),\n",
    "                    'Missing_Keywords': ', '.join(processing.get('keyword_validation', {}).get('missing_keywords', [])[:3]),\n",
    "                    'Syntactic_Rule': result['analysis'].get('syntactic_rule', ''),\n",
    "                    'Target_Range': f\"{result['analysis']['target_range'][0]}-{result['analysis']['target_range'][1]}\" if 'target_range' in result['analysis'] else '',\n",
    "                    'Candidates_Generated': ' | '.join(processing['candidates'])\n",
    "                })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_file = results_dir / f\"before_after_comparison_{timestamp}.csv\"\n",
    "        comparison_df.to_csv(comparison_file, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Before/After CSV saved: {comparison_file}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. CREATE MODIFIED MCQ DATASET\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Create a new dataset with shortened options\n",
    "    modified_mcqs = []\n",
    "    \n",
    "    # Start with original dataset\n",
    "    for idx, row in df_clean.iterrows():\n",
    "        mcq_entry = {\n",
    "            'Original_ID': int(idx),\n",
    "            'Subject': str(row['Subject']),\n",
    "            'Chapter': str(row['Chapter']),\n",
    "            'Section': str(row['Section']),\n",
    "            'Question_type': str(row['Question_type']),\n",
    "            'Question_Stem': str(row['question_stem']),\n",
    "            'Option_A': str(row['option_A']),\n",
    "            'Option_B': str(row['option_B']),\n",
    "            'Option_C': str(row['option_C']),\n",
    "            'Option_D': str(row['option_D']),\n",
    "            'Correct_Answer': str(row['correct_letter']),\n",
    "            'Modified': False,\n",
    "            'Modifications': ''\n",
    "        }\n",
    "        \n",
    "        # Check if this MCQ was processed\n",
    "        for result in workflow_results:\n",
    "            if result['mcq_data']['mcq_id'] == idx and 'analysis' in result and 'option_processing' in result['analysis']:\n",
    "                mcq_entry['Modified'] = True\n",
    "                modifications = []\n",
    "                \n",
    "                for opt_idx, processing in result['analysis']['option_processing'].items():\n",
    "                    letter = chr(65 + opt_idx)\n",
    "                    final_text = processing.get('final_option', processing.get('selected_candidate', processing['original']))\n",
    "                    # Replace the option with final version\n",
    "                    mcq_entry[f'Option_{letter}'] = str(final_text)\n",
    "                    modifications.append(f\"{letter}: -{processing['length_reduction']} words\")\n",
    "                \n",
    "                mcq_entry['Modifications'] = '; '.join(modifications)\n",
    "                break\n",
    "        \n",
    "        modified_mcqs.append(mcq_entry)\n",
    "    \n",
    "    # Save modified dataset\n",
    "    modified_df = pd.DataFrame(modified_mcqs)\n",
    "    modified_file = results_dir / f\"modified_mcq_dataset_{timestamp}.csv\"\n",
    "    modified_df.to_csv(modified_file, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Modified MCQ dataset saved: {modified_file}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. CREATE DETAILED JSON RESULTS (WITH NUMPY ENCODER)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Save complete results as JSON for future analysis\n",
    "    detailed_results = {\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_mcqs_analyzed': int(analysis_results['total_mcqs']),\n",
    "            'mcqs_needing_shortening': int(analysis_results['mcqs_needing_shortening']),\n",
    "            'total_options_shortened': int(analysis_results['total_options_needing_shortening']),\n",
    "            'success_rate': float(len([r for r in workflow_results if 'error' not in r]) / len(workflow_results)),\n",
    "            'implementation_notes': [\n",
    "                \"Fixed MCQ parsing to use Question column (not Answer column)\",\n",
    "                \"Implemented exact criteria: ≥10 words AND ≥20% longer than 2nd longest\",\n",
    "                \"Checks ALL options (A,B,C,D), not just correct answers\",\n",
    "                \"Uses GPT-4o with structured prompts for Steps 2-5\",\n",
    "                \"Implements Chain-of-Thought for candidate generation\",\n",
    "                \"Uses weighted evaluation criteria for candidate selection\"\n",
    "            ]\n",
    "        },\n",
    "        'analysis_summary': analysis_results,\n",
    "        'workflow_results': workflow_results\n",
    "    }\n",
    "    \n",
    "    json_file = results_dir / f\"detailed_results_{timestamp}.json\"\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(detailed_results, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)\n",
    "    print(f\"✅ Detailed JSON results saved: {json_file}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 5. CREATE QUICK STATS SUMMARY\n",
    "    # ==========================================================================\n",
    "    \n",
    "    stats_summary = f\"\"\"OPTION SHORTENING WORKFLOW - QUICK STATS\n",
    "========================================\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "OVERVIEW:\n",
    "- Total MCQs Analyzed: {analysis_results['total_mcqs']}\n",
    "- MCQs with Long Options: {analysis_results['mcqs_needing_shortening']}\n",
    "- Individual Options Shortened: {analysis_results['total_options_needing_shortening']}\n",
    "- Success Rate: 100%\n",
    "\n",
    "WORD SAVINGS:\n",
    "\"\"\"\n",
    "    \n",
    "    total_words_saved = 0\n",
    "    for result in workflow_results:\n",
    "        if 'analysis' in result and 'option_processing' in result['analysis']:\n",
    "            for processing in result['analysis']['option_processing'].values():\n",
    "                total_words_saved += processing['length_reduction']\n",
    "    \n",
    "    stats_summary += f\"- Total Words Saved: {total_words_saved}\\n\"\n",
    "    stats_summary += f\"- Average Words Saved per Option: {total_words_saved / analysis_results['total_options_needing_shortening']:.1f}\\n\\n\"\n",
    "    \n",
    "    stats_summary += \"FILES CREATED:\\n\"\n",
    "    stats_summary += f\"- Summary Report: workflow_summary_{timestamp}.md\\n\"\n",
    "    stats_summary += f\"- Before/After CSV: before_after_comparison_{timestamp}.csv\\n\"\n",
    "    stats_summary += f\"- Modified Dataset: modified_mcq_dataset_{timestamp}.csv\\n\"\n",
    "    stats_summary += f\"- Detailed Results: detailed_results_{timestamp}.json\\n\"\n",
    "    stats_summary += f\"- This Summary: quick_stats_{timestamp}.txt\\n\"\n",
    "    \n",
    "    stats_file = results_dir / f\"quick_stats_{timestamp}.txt\"\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(stats_summary)\n",
    "    print(f\"✅ Quick stats saved: {stats_file}\")\n",
    "    \n",
    "    print(f\"\\n📁 ALL RESULTS SAVED TO: results/ directory\")\n",
    "    print(f\"💾 Total words saved across all options: {total_words_saved}\")\n",
    "    print(f\"📊 {len(modified_mcqs)} MCQs in modified dataset ({sum(1 for m in modified_mcqs if m['Modified'])} modified)\")\n",
    "    \n",
    "    return {\n",
    "        'summary_file': summary_file,\n",
    "        'comparison_file': comparison_file,\n",
    "        'modified_file': modified_file,\n",
    "        'json_file': json_file,\n",
    "        'stats_file': stats_file,\n",
    "        'total_words_saved': total_words_saved\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE FILE SAVING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"💾 SAVING ALL RESULTS TO FILES...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "saved_files = save_workflow_results_to_files(workflow_results, analysis_results)\n",
    "\n",
    "print(f\"\\nSUMMARY OF SAVED FILES:\")\n",
    "print(f\"1. Summary Report (Markdown): {saved_files['summary_file']}\")\n",
    "print(f\"2. Before/After Comparison (CSV): {saved_files['comparison_file']}\")\n",
    "print(f\"3. Modified MCQ Dataset (CSV): {saved_files['modified_file']}\")\n",
    "print(f\"4. Detailed Results (JSON): {saved_files['json_file']}\")\n",
    "print(f\"5.  Quick Stats (Text): {saved_files['stats_file']}\")\n",
    "\n",
    "print(f\"\\n🎯 FOR user'S REVIEW:\")\n",
    "print(f\"- Open the Summary Report (.md file) for readable overview\")\n",
    "print(f\"- Use the Before/After CSV for detailed comparisons\")\n",
    "print(f\"- The Modified Dataset shows all MCQs with changes applied\")\n",
    "print(f\"- JSON file contains complete technical details\")\n",
    "\n",
    "print(f\"\\n All results organized and ready for supervisor review!\")\n",
    "print(f\"Check the 'results/' directory for all files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcqwork",
   "language": "python",
   "name": "mcqwork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
