{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_sm is already installed.\n"
     ]
    }
   ],
   "source": [
    "from src.text_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in data folder\n",
    "files_to_process =  get_files_in_directory(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file('../data/text3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = add_chunk_markers(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.planner import generate_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique invocation ID for this workflow run\n",
    "invocation_id = str(uuid.uuid4())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id=\"session_12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 09:31:09,376 - INFO - generate_plan invoked: invocation_id=266ae392-2ab3-4b13-ba23-83d09a0b1f56 model=gpt-4o fact=1 inference=1 text_len=2764\n",
      "2025-10-21 09:31:09,378 - INFO - Table 'plan_metadata' already exists.\n",
      "2025-10-21 09:31:09,384 - INFO - Planner agent initialized with model: gpt-4o\n",
      "2025-10-21 09:31:21,666 - INFO - HTTP Request: POST https://api-main-poc.aiml.asu.edu/queryV2 \"HTTP/1.1 200 OK\"\n",
      "2025-10-21 09:31:21,669 - INFO - Parsed plan JSON successfully (facts=1, inferences=1)\n",
      "2025-10-21 09:31:21,673 - INFO - Metadata inserted into 'plan_metadata'.\n",
      "2025-10-21 09:31:21,675 - INFO - Plan metadata inserted into DB: table=plan_metadata invocation_id=266ae392-2ab3-4b13-ba23-83d09a0b1f56\n"
     ]
    }
   ],
   "source": [
    "plan = await generate_plan(\n",
    "    session_id=session_id,\n",
    "    invocation_id=invocation_id,\n",
    "    model=\"gpt-4o\",\n",
    "    text=chunked_text, \n",
    "    fact = 1, \n",
    "    inference = 1, \n",
    "    table_name = \"plan_metadata\", \n",
    "    database_file = \"../database/mcq_metadata.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 'session_12345',\n",
       " 'system_prompt': '[ROLE]:\\n  You are an experienced college instructor specializing in developing reading comprehension questions to evaluate students\\' understanding of academic texts, such as textbook chapters and academic articles. You excel at identifying and selecting key facts and inferences from these texts for question development.\\n\\n[TASK]:\\n  Your task is to summarize the academic text provided by the user and select the specified number of facts and inferences as requested by the user following the guidelines below. \\n\\n[Guidelines]\\n  Step1: Summarize the text \\n  The text is divided into chunks (as indicated by the html tags such as <chunk1></chunk1>). Summarize the content within each chunk, maintaining the chunk tags in your summary.\\n\\n  Step2: Identify key facts and/or inferences\\n  Depending on the user’s requests, create a list of key facts and/or inferences from the academic text. The number of facts or inferences generated in this step can be more than what the user requests. \\n  \\n  Read the definitions below to better understand what key facts and inferences are:\\n  \\n  ** Key Facts**\\n  Key facts are explicit pieces of information stated directly in the text. They usually reflect the main ideas, supporting details, definitions, results, or statistics that are important for understanding the author’s argument or findings.\\n \\n  ** Key Inferences**\\n  Key inferences are logical conclusions that a reader can draw from the information in the text, even though they are not directly stated. Inferences require the reader to interpret, analyze, or synthesize information to understand implications, underlying meanings, or cause-effect relationships.\\n  Characteristics:\\n  - Require higher-order thinking (interpretive or inferential comprehension)\\n  - Often answer the questions: why, what does this suggest, what can be concluded, what would likely happen if...\\n  Example (from a sociology article):\\n  \"In communities with high income inequality, researchers found lower levels of trust among residents and reduced participation in civic activities.\"\\n  Key inference: High income inequality may contribute to weakened social cohesion.\\n\\n  key inferences may emerge from synthesizing ideas spread across multiple sentences or paragraphs. These inferences require readers to connect concepts that are not directly adjacent in the text.\\n  Example (from a Psychology Text):\\n  chunk1: \"...Studies show that securely attached infants tend to be more socially competent and exhibit fewer behavioral problems later in life...\"\\n  chunk5: \"...Longitudinal research indicates that early childhood experiences with caregivers shape patterns of emotion regulation, social skills, and even academic performance into adolescence and adulthood...\"\\n  Key inference: Children who experience secure attachment and authoritative parenting early in life are more likely to develop strong emotional and social skills that persist into adolescence and adulthood.\\n\\n  **Principles for Identifying Key Facts and Inferences**\\n    - Focus on Central Ideas: Identify the main argument, thesis, or purpose of the text.\\n    - Highlight Supporting Information: Look for facts, evidence, definitions, or examples that directly support the main ideas.\\n    - Pay Attention to Logical Relationships: Note cause-effect, comparison, contrast, and sequence relationships that organize the information.\\n    - Recognize Implied Meaning: Infer what the author suggests but does not say directly—such as assumptions, tone, or implications.\\n    - Ignore peripheral details: Exclude minor details (e.g., time, location), anecdotes, or examples that do not contribute significantly to the overall understanding of the text.\\n    - Prioritize What’s Essential for Understanding: Choose facts and inferences that are crucial for grasping the overall meaning or structure of the text.\\n    - Avoid redundancy: Do not include the same fact or inference multiple times, even if it appears in different chunks.\\n    - Pay special attention to Learning/Teaching Objectives if they are provided in the text, as they often highlight the most important concepts or skills that the text aims to convey. \\n  \\n  Step3: Order the list of facts or/and inferences based on their importance for understanding the text. Select the desired number of facts or/and inferences as requested by the user. \\n  \\n  Step4: Output your response following the JSON format as specified below. \\n\\n  {\\n    \"summary\": \"Summarized text here\",\\n    \"reasoning_for_selection\": \"Your reasoning for the selection here\",\\n    \"selection\": {\\n      \"facts\": {\\n        \"fact1\": {\"content\": \"Content of fact1\", \"chunk\": [chunk_label]},\\n        \"fact2\": {\"content\": \"Content of fact2\", \"chunk\": [chunk_label]}\\n        // Add more facts as requested by the user\\n      },\\n      \"inferences\": {\\n        \"inference1\": {\"content\": \"Content of inference1\", \"chunk\": [chunk_label]},\\n        \"inference2\": {\"content\": \"Content of inference2\", \"chunk\": [chunk_label]}\\n        // Add more inferences as requested by the user\\n      }\\n    }\\n  }\\n\\n  IMPORTANT:\\n  - The \"reasoning_for_selection\" should briefly explain how the facts or inferences are derived and why they are important for understanding the text.\\n  - The number of facts and inferences should match the user\\'s request. For example, if the user requests 3 facts, include \"fact1\", \"fact2\", and \"fact3\".\\n  - If no facts or inferences are requested, the values for the \"facts\" or \"inferences\" field should be an empty object {}.\\n  - The \"chunk\" field indicates the source of the facts or inferences in the text. The value is a list of chunk labels obtained from the html tags, such as “chunk1”, “chunk2”. Sometimes an inference may be from multiple chunks, such as [“chunk1”, “chunck2”].\\n  - Return ONLY a JSON object matching the JSON format specified above. No prose, no explanations, no code fences.\\n\\n\\nStep5: Self-critique your response\\n  After completing the task, review your response to ensure it meets the requirements. Revise your output if necessary. \\n  - Do you keep the chunk tags in the summary? \\n  - Do you provide the number and type of facts and/or inferences as requested by the user?\\n  - Are the facts and/or inferences relevant and essential for comprehending the text? \\n  - Do you provide correct chunk labels to indicate the source of each fact and/or inference?\\n  - Does the output include everything required and follow the format correctly?\\n',\n",
       " 'user_prompt': '<text>\\n<chunk1>Research magazine from Stanford University Journalist Anne Simonson â€“ January 13, 2008 The sun counteracts cancer\\nSunrays that hit our skin reinforce the bodyâ€™s own defense against cancer. The campaigns to get people out of the sun have gone too far, according to the researcher Johan Moan. â€œThe rays of the sun speed up the bodyâ€™s production of vitamin D. This can prevent cancer as well as many other diseases.â€\\x9d So says Johan Moan, Professor at the Cancer Center at Boston Medical Center. Along with colleagues from Norway, he is presenting results that support the theory that sun exposure provides the body with a more effective defense against cancer. The findings show, for example, that people who live closer to the equator, and whose bodies hence are more exposed to the sun, have a better chance of survival if they develop cancer in their internal organs. Moreover, they show that cancer patients have a better prognosis if they are diagnosed in summer or autumn, when their store of vitamin D has been replenished. Their research has been published in the prestigious scientific journal Proceedings of the National Academy of Sciences. Sunbathing or cod-liver oil is important to northerners\\nVitamin D has also been shown to play a protective role in many types of cancer of internal organs, and possibly also for a number of other diseases. Avoiding the sun can thus do more harm than good, according to Richard Setlow, who is one of Moanâ€™s collaborators. He thinks that this applies especially to us northerners, who see little of the sun for long periods of the year. Setlow is a biophysicist at the Brookhaven National Laboratory and is one of the worldâ€™s most recognized experts on the relationship between solar radiation and skin cancer.</chunk1>\\n\\n<chunk2>â€œPeople are being advised to spend less time in the sun, but some sun without sunscreen is healthy for us. If you stay outside for half an hour in midsummer, you obtain the same amount of vitamin D as you would from drinking a small bottle of cod-liver oil,â€\\x9d Moan says. He claims that in addition, it would be beneficial to use a tanning bed once or twice a week during the dark winter months, because this provides an excellent supplement of vitamin D at that time of year. â€œFrom November to March we obtain no vitamin D from the sun, and we therefore need to ingest it or obtain it from a tanning bed,â€\\x9d Moan explains. Melanomas are triggered by the type of solar radiation known as UVA. The bodyâ€™s production of vitamin D is stimulated by the type of solar radiation known as UVB. Perhaps we should change the sunscreens so that they do not block as much of the UVB radiation, while still protecting us against the UVA rays, according to Setlow.\\n</chunk2>\\n</text>\\n<number_of_facts>\\nI need 1 key facts from the text.\\n</number_of_facts>\\n<number_of_inferences>\\nI need 1 key inferences from the text.\\n</number_of_inferences>',\n",
       " 'model': 'gpt-4o',\n",
       " 'completion': '{\\n  \"summary\": {\\n    \"chunk1\": \"Research magazine from Stanford University Journalist Anne Simonson – January 13, 2008 The sun counteracts cancer. Sunrays that hit our skin reinforce the body’s own defense against cancer. The campaigns to get people out of the sun have gone too far, according to the researcher Johan Moan. “The rays of the sun speed up the body’s production of vitamin D. This can prevent cancer as well as many other diseases.” Johan Moan, a Professor at the Cancer Center at Boston Medical Center, along with colleagues from Norway, presents results supporting the theory that sun exposure enhances the body’s defense against cancer. Findings indicate that individuals living closer to the equator, who receive more sun exposure, have a better chance of survival if they develop internal organ cancer. Additionally, cancer patients diagnosed in summer or autumn, when vitamin D levels are higher, have a better prognosis. Their research is published in the Proceedings of the National Academy of Sciences. Vitamin D plays a protective role in various internal organ cancers and possibly other diseases. Richard Setlow, a collaborator of Moan, argues that avoiding the sun can be harmful, especially for northerners who experience limited sun exposure for extended periods.\",\\n    \"chunk2\": \"“People are being advised to spend less time in the sun, but some sun without sunscreen is healthy for us. If you stay outside for half an hour in midsummer, you obtain the same amount of vitamin D as you would from drinking a small bottle of cod-liver oil,” Moan states. He suggests that using a tanning bed once or twice a week during winter months can provide a good vitamin D supplement. “From November to March we obtain no vitamin D from the sun, and we therefore need to ingest it or obtain it from a tanning bed,” Moan explains. Melanomas are caused by UVA radiation, while vitamin D production is stimulated by UVB radiation. Setlow proposes that sunscreens should be modified to block less UVB radiation while still protecting against UVA.\"\\n  },\\n  \"reasoning_for_selection\": \"The selected fact and inference are essential for understanding the text\\'s argument about the health benefits of sun exposure and vitamin D production, as well as the implications of current sun safety recommendations.\",\\n  \"selection\": {\\n    \"facts\": {\\n      \"fact1\": {\\n        \"content\": \"Sun exposure increases the body\\'s production of vitamin D, which can prevent cancer and other diseases.\",\\n        \"chunk\": [\"chunk1\"]\\n      }\\n    },\\n    \"inferences\": {\\n      \"inference1\": {\\n        \"content\": \"Current recommendations to avoid sun exposure may inadvertently increase the risk of cancer by reducing vitamin D levels.\",\\n        \"chunk\": [\"chunk1\", \"chunk2\"]\\n      }\\n    }\\n  }\\n}',\n",
       " 'execution_time': '0:00:12.283034',\n",
       " 'input_tokens': 5420,\n",
       " 'output_tokens': 576,\n",
       " 'invocation_id': '266ae392-2ab3-4b13-ba23-83d09a0b1f56',\n",
       " 'summary': '{\"chunk1\": \"Research magazine from Stanford University Journalist Anne Simonson \\\\u2013 January 13, 2008 The sun counteracts cancer. Sunrays that hit our skin reinforce the body\\\\u2019s own defense against cancer. The campaigns to get people out of the sun have gone too far, according to the researcher Johan Moan. \\\\u201cThe rays of the sun speed up the body\\\\u2019s production of vitamin D. This can prevent cancer as well as many other diseases.\\\\u201d Johan Moan, a Professor at the Cancer Center at Boston Medical Center, along with colleagues from Norway, presents results supporting the theory that sun exposure enhances the body\\\\u2019s defense against cancer. Findings indicate that individuals living closer to the equator, who receive more sun exposure, have a better chance of survival if they develop internal organ cancer. Additionally, cancer patients diagnosed in summer or autumn, when vitamin D levels are higher, have a better prognosis. Their research is published in the Proceedings of the National Academy of Sciences. Vitamin D plays a protective role in various internal organ cancers and possibly other diseases. Richard Setlow, a collaborator of Moan, argues that avoiding the sun can be harmful, especially for northerners who experience limited sun exposure for extended periods.\", \"chunk2\": \"\\\\u201cPeople are being advised to spend less time in the sun, but some sun without sunscreen is healthy for us. If you stay outside for half an hour in midsummer, you obtain the same amount of vitamin D as you would from drinking a small bottle of cod-liver oil,\\\\u201d Moan states. He suggests that using a tanning bed once or twice a week during winter months can provide a good vitamin D supplement. \\\\u201cFrom November to March we obtain no vitamin D from the sun, and we therefore need to ingest it or obtain it from a tanning bed,\\\\u201d Moan explains. Melanomas are caused by UVA radiation, while vitamin D production is stimulated by UVB radiation. Setlow proposes that sunscreens should be modified to block less UVB radiation while still protecting against UVA.\"}',\n",
       " 'facts': '{\"fact1\": {\"content\": \"Sun exposure increases the body\\'s production of vitamin D, which can prevent cancer and other diseases.\", \"chunk\": [\"chunk1\"]}}',\n",
       " 'inferences': '{\"inference1\": {\"content\": \"Current recommendations to avoid sun exposure may inadvertently increase the risk of cancer by reducing vitamin D levels.\", \"chunk\": [\"chunk1\", \"chunk2\"]}}',\n",
       " 'timestamp': '2025-10-21T09:31:21.669270'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_plan object: <function generate_plan at 0x000001BE81221990>\n",
      "module: src.planner\n",
      "signature: (invocation_id: str, model: str, text: str, fact: int, inference: int, table_name: str = 'plan_metadata', database_file: str = '../database/mcq_metadata.db') -> dict\n",
      "source (first 200 chars):\n",
      "async def generate_plan(invocation_id: str,\n",
      "                  model: str,\n",
      "                  text: str, \n",
      "                  fact: int, \n",
      "                  inference: int, \n",
      "                  table_name:st\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(\"generate_plan object:\", generate_plan)\n",
    "print(\"module:\", getattr(generate_plan, \"__module__\", None))\n",
    "print(\"signature:\", inspect.signature(generate_plan))\n",
    "print(\"source (first 200 chars):\")\n",
    "src = inspect.getsource(generate_plan)\n",
    "print(src[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.controller_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact =1\n",
    "inference = 1\n",
    "main_idea = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 09:33:09,898 - INFO - create_task_list called (requested: facts=1, inferences=1, main_idea=1)\n",
      "2025-10-21 09:33:09,899 - INFO - validate_and_parse_plan called (expected facts=1, inferences=1)\n",
      "2025-10-21 09:33:09,901 - INFO - Counts — facts: expected=1 actual=1; inferences: expected=1 actual=1\n",
      "2025-10-21 09:33:09,902 - INFO - Validation passed. Parsed summary_len=2087, facts=1, inferences=1\n",
      "2025-10-21 09:33:09,904 - INFO - Validated plan: facts=1, inferences=1\n",
      "2025-10-21 09:33:09,905 - INFO - extract_chunks called with 1 chunk label(s)\n",
      "2025-10-21 09:33:09,906 - INFO - extract_chunks returning text_len=1770\n",
      "2025-10-21 09:33:09,908 - INFO - extract_unlisted_chunks called; excluding 1 chunk label(s)\n",
      "2025-10-21 09:33:09,909 - INFO - extract_unlisted_chunks kept 0 segment(s)\n",
      "2025-10-21 09:33:09,910 - INFO - extract_chunks called with 2 chunk label(s)\n",
      "2025-10-21 09:33:09,911 - INFO - extract_chunks returning text_len=2728\n",
      "2025-10-21 09:33:09,912 - INFO - extract_unlisted_chunks called; excluding 2 chunk label(s)\n",
      "2025-10-21 09:33:09,913 - INFO - extract_unlisted_chunks kept 0 segment(s)\n",
      "2025-10-21 09:33:09,913 - INFO - extract_summary called with text_len=2087\n",
      "2025-10-21 09:33:09,914 - INFO - extract_summary returning text_len=2087\n",
      "2025-10-21 09:33:09,915 - INFO - create_task_list built 3 task(s) total\n"
     ]
    }
   ],
   "source": [
    "task_list = create_task_list(chunked_text, plan, fact, inference, main_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fact'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = task_list[0]\n",
    "task.get(\"question_type\", \"\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MCQ Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytian126\\Documents\\repos\\MCQ-generation\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.mcq_generation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_all_mcqs() missing 1 required positional argument: 'invocation_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_all_mcqs(task_list, invocation_id, \n\u001b[0;32m      2\u001b[0m                                     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                     mcq_metadata_table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcq_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      4\u001b[0m                                     evaluation_metadata_table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                     database_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../database/mcq_metadata.db\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                     max_attempt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                     concurrency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                     )\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_all_mcqs() missing 1 required positional argument: 'invocation_id'"
     ]
    }
   ],
   "source": [
    "questions = await generate_all_mcqs(session_id= session_id, task_list, invocation_id, \n",
    "                                    model=\"gpt-4o\", \n",
    "                                    mcq_metadata_table_name=\"mcq_metadata\", \n",
    "                                    evaluation_metadata_table_name=\"evaluation_metadata\",\n",
    "                                    database_file=\"../database/mcq_metadata.db\",\n",
    "                                    max_attempt=3,\n",
    "                                    concurrency=30,\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = await generate_all_mcqs_quality_first(task_list, \n",
    "                                                  invocation_id, \n",
    "                                    model=\"gpt-4o\", \n",
    "                                    mcq_metadata_table_name=\"mcq_metadata\", \n",
    "                                    evaluation_metadata_table_name=\"evaluation_metadata\",\n",
    "                                    ranking_metadata_table_name=\"ranking_metadata\",\n",
    "                                    database_file=\"../database/mcq_metadata.db\",\n",
    "                                    max_attempt=3,\n",
    "                                    candidate_num=5,\n",
    "                                    concurrency=30,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Quality-first MCQ generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_metadata = await generate_mcq_quality_first(\n",
    "    invocation_id=invocation_id,\n",
    "    model=\"gpt-4o\",\n",
    "    task=task_list[0],\n",
    "    mcq_table_name=\"mcq_metadata\",\n",
    "    ranking_table_name=\"ranking_metadata\",\n",
    "    database_file='../database/mcq_metadata.db',\n",
    "    max_attempt = 3,\n",
    "    attempt= 1,\n",
    "    candidate_num = 5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.formatter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_questions = reformat_mcq_metadata(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Full Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.general import *\n",
    "\n",
    "# get all the files in data folder\n",
    "files_to_process =  get_files_in_directory(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file('../data/text3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.workflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await question_generation_workflow(text, \n",
    "                                            fact = 1, \n",
    "                                            inference = 1, \n",
    "                                            main_idea = 1, \n",
    "                                            model = \"gpt-5\", \n",
    "                                            quality_first = True, \n",
    "                                            candidate_num =5,\n",
    "                                            max_attempt_for_single_mcq = 3,\n",
    "                                            plan_metadata_table_name = \"plan_metadata\",\n",
    "                                            mcq_metadata_table_name = \"mcq_metadata\",\n",
    "                                            evaluation_metadata_table_name = \"evaluation_metadata\",\n",
    "                                            ranking_metadata_table_name = \"ranking_metadata\",\n",
    "                                            workflow_metadata_table_name = \"workflow_metadata\", \n",
    "                                            database_file = \"../database/mcq_metadata.db\",\n",
    "                                            concurrency = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the output from database to a CSV file \n",
    "from src.database_handler import export_table_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_table_to_csv(\n",
    "    table_name=\"mcq_metadata\",\n",
    "    file_name=\"20250530_mcq_metadata.csv\",\n",
    "    database_file=\"../database/mcq_metadata.db\",\n",
    "\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
