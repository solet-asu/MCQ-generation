{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf10a814",
   "metadata": {},
   "source": [
    "### Wrangle the questions so that they can be ingested into the web application in certain format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06c2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Your robust MCQ parser\n",
    "from src.general import extract_mcq_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffa2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- File paths (edit if needed)\n",
    "baseline_input = \"./some_recorded_output/baseline_questions_cleaned.xlsx\"\n",
    "requesta_input = \"./some_recorded_output/requesta_mcqs_cleaned.xlsx\"\n",
    "attention_input = \"./some_recorded_output/attention_checker.xlsx\"\n",
    "\n",
    "baseline_output = \"./some_recorded_output/baseline_questions_cleaned_parsed.xlsx\"\n",
    "requesta_output = \"./some_recorded_output/requesta_mcqs_cleaned_parsed.xlsx\"\n",
    "attention_output = \"./some_recorded_output/attention_checker_parsed.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b812709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ✅\n",
      "Baseline → ./some_recorded_output/baseline_questions_cleaned_parsed.xlsx\n",
      "ReQUESTA → ./some_recorded_output/requesta_mcqs_cleaned_parsed.xlsx\n",
      "Attention → ./some_recorded_output/attention_checker_parsed.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Utilities\n",
    "def _safe_extract(q: object) -> Tuple[str, List[Optional[str]]]:\n",
    "    \"\"\"Wrap extract_mcq_components with a safety net so a bad row never breaks the run.\"\"\"\n",
    "    try:\n",
    "        stem, opts = extract_mcq_components(q)\n",
    "        # Normalize empty strings to None for options\n",
    "        norm_opts = [o if (o is not None and str(o).strip() != \"\") else None for o in (opts or [None]*4)]\n",
    "        # Ensure exactly 4 options (A–D)\n",
    "        if len(norm_opts) < 4:\n",
    "            norm_opts = norm_opts + [None] * (4 - len(norm_opts))\n",
    "        elif len(norm_opts) > 4:\n",
    "            norm_opts = norm_opts[:4]\n",
    "        return stem, norm_opts\n",
    "    except Exception as e:\n",
    "        # You may want to log/print e for debugging\n",
    "        return \"\", [None, None, None, None]\n",
    "\n",
    "\n",
    "def add_parsed_columns(df: pd.DataFrame, question_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add parsed MCQ columns to df:\n",
    "      - question_item (stem)\n",
    "      - optionA, optionB, optionC, optionD\n",
    "    \"\"\"\n",
    "    if question_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{question_col}' not found in DataFrame.\")\n",
    "\n",
    "    # Apply parsing\n",
    "    parsed = df[question_col].apply(_safe_extract)\n",
    "\n",
    "    # Split into stem + options\n",
    "    stems = parsed.map(lambda t: t[0])\n",
    "    options = parsed.map(lambda t: t[1])\n",
    "\n",
    "    opt_df = pd.DataFrame(\n",
    "        options.tolist(),\n",
    "        columns=[\"optionA\", \"optionB\", \"optionC\", \"optionD\"],\n",
    "        index=df.index,\n",
    "    )\n",
    "\n",
    "    # Assemble the result\n",
    "    out = df.copy()\n",
    "    out[\"question_item\"] = stems\n",
    "    out[[\"optionA\", \"optionB\", \"optionC\", \"optionD\"]] = opt_df\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ensure_parent_dir(path_str: str | Path) -> None:\n",
    "    p = Path(path_str)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1) Read the three xlsx files\n",
    "read_kwargs = dict(engine=\"openpyxl\")  # explicit engine for reliability\n",
    "\n",
    "baseline_df = pd.read_excel(baseline_input, **read_kwargs)\n",
    "requesta_df = pd.read_excel(requesta_input, **read_kwargs)\n",
    "attention_df = pd.read_excel(attention_input, **read_kwargs)\n",
    "\n",
    "# --- 2) Parse stems & options from the three question columns\n",
    "baseline_parsed = add_parsed_columns(baseline_df, \"baseline_question\")\n",
    "requesta_parsed = add_parsed_columns(requesta_df, \"requesta_question\")\n",
    "attention_parsed = add_parsed_columns(attention_df, \"attention_question\")\n",
    "\n",
    "# --- 3) (already added) columns: question_item, optionA–optionD in each DataFrame\n",
    "\n",
    "# --- 4) Write outputs\n",
    "_ensure_parent_dir(baseline_output)\n",
    "_ensure_parent_dir(requesta_output)\n",
    "_ensure_parent_dir(attention_output)\n",
    "\n",
    "baseline_parsed.to_excel(baseline_output, index=False, engine=\"openpyxl\")\n",
    "requesta_parsed.to_excel(requesta_output, index=False, engine=\"openpyxl\")\n",
    "attention_parsed.to_excel(attention_output, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Done ✅\")\n",
    "print(f\"Baseline → {baseline_output}\")\n",
    "print(f\"ReQUESTA → {requesta_output}\")\n",
    "print(f\"Attention → {attention_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe232a",
   "metadata": {},
   "source": [
    "### Extract data from the xlsx files and ingest them into dictionaries for web application use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003fcb1",
   "metadata": {},
   "source": [
    "### First, Passage dictionary store all the 20 passages with their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddddfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a587d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_XLSX = Path(\"./some_recorded_output/baseline_questions_cleaned_parsed.xlsx\")\n",
    "\n",
    "OUTPUT_TXT = Path(\"./some_recorded_output/all_passages_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph split on blank lines (one or more)\n",
    "RE_BLANKLINE = re.compile(r\"\\n\\s*\\n\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def normalize_paragraphs(text: str) -> str:\n",
    "    \"\"\"Ensure paragraphs are separated by exactly one '\\n\\n' and no trailing blank line.\n",
    "    - Keep the original paragraph order\n",
    "    - Normalize CRLF/CR to LF\n",
    "    - Convert non-breaking spaces to regular spaces\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = text.replace(\"\\u00A0\", \" \")  # NBSP -> space\n",
    "\n",
    "    parts = [p.strip() for p in RE_BLANKLINE.split(text.strip()) if p.strip()]\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def load_unique_texts(xlsx_path: Path, n: int = 20) -> List[Tuple[str, str]]:\n",
    "    # .xlsx is Unicode-aware; no encoding arg needed here\n",
    "    df = pd.read_excel(xlsx_path, engine=\"openpyxl\", dtype={\"textID\": str})\n",
    "\n",
    "    required_cols = {\"textID\", \"text\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in {xlsx_path.name}: {sorted(missing)}\"\n",
    "        )\n",
    "\n",
    "    df = df[[\"textID\", \"text\"]].dropna(subset=[\"textID\", \"text\"]).copy()\n",
    "\n",
    "    # De-duplicate by textID (keep first occurrence, preserve order), then take first n\n",
    "    df = df.loc[~df[\"textID\"].duplicated(keep=\"first\")].head(n)\n",
    "\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    for _, row in df.iterrows():\n",
    "        tid = str(row[\"textID\"]).strip()\n",
    "        txt = normalize_paragraphs(row[\"text\"])\n",
    "        pairs.append((tid, txt))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def build_passages(pairs: List[Tuple[str, str]]) -> Dict[str, Dict[str, str]]:\n",
    "    passages: Dict[str, Dict[str, str]] = {}\n",
    "    for idx, (tid, txt) in enumerate(pairs, start=1):\n",
    "        key = f\"p{idx}\"\n",
    "        passages[key] = {\n",
    "            \"id\": tid,\n",
    "            \"title\": \"Passage\",\n",
    "            \"text\": txt,\n",
    "        }\n",
    "        \n",
    "    return passages\n",
    "\n",
    "\n",
    "def _escape_triple_quotes(s: str) -> str:\n",
    "    return s.replace('\"\"\"', '\\\\\"\"\"')\n",
    "\n",
    "\n",
    "def _escape_single_quotes(s: str) -> str:\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
    "\n",
    "\n",
    "def write_txt(passages: Dict[str, Dict[str, str]], out_path: Path) -> None:\n",
    "    \"\"\"Write a Python-friendly dict literal to a .txt file, using triple-quoted\n",
    "    multi-line strings for the passage texts. File is saved as UTF-8.\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"PASSAGES = {\\n\")\n",
    "        for key, item in passages.items():\n",
    "            f.write(f\"    '{key}': {{\\n\")\n",
    "            f.write(f\"        'id': '{_escape_single_quotes(item['id'])}',\\n\")\n",
    "            f.write(\"        'title': 'Passage',\\n\")\n",
    "            f.write(\"        'text': \\\"\\\"\\\"\\n\")\n",
    "            f.write(_escape_triple_quotes(item[\"text\"]))\n",
    "            f.write(\"\\n\\\"\\\"\\\"\\n\")\n",
    "            f.write(\"    },\\n\")\n",
    "        f.write(\"}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def main(n: int = 20) -> Dict[str, Dict[str, str]]:\n",
    "    xlsx_path = INPUT_XLSX\n",
    "    pairs = load_unique_texts(xlsx_path, n=n)\n",
    "    passages = build_passages(pairs)\n",
    "    write_txt(passages, OUTPUT_TXT)\n",
    "\n",
    "    # Notebook-friendly summary\n",
    "    print(f\"Wrote {len(passages)} passages to {OUTPUT_TXT}\")\n",
    "    for k in list(passages.keys())[:3]:\n",
    "        print(f\"  {k}: id={passages[k]['id']}\")\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f052cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 20 passages to some_recorded_output\\all_passages_dict.txt\n",
      "  p1: id=anthropology_1_2\n",
      "  p2: id=anthropology_1_3\n",
      "  p3: id=anthropology_2_3\n"
     ]
    }
   ],
   "source": [
    "# Run in a notebook cell\n",
    "PASSAGES = main(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beee300",
   "metadata": {},
   "source": [
    "### Second, extract information from relevant files and ingest it to QUESTION dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b24fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import random\n",
    "from pprint import pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92608acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config ----------\n",
    "DIR = Path(\"./some_recorded_output\")\n",
    "PASSAGES_TXT = DIR / \"all_passages_dict.txt\"       # plain dict literal {...}\n",
    "BASELINE_XLSX = DIR / \"baseline_questions_cleaned_parsed.xlsx\"\n",
    "REQUESTA_XLSX = DIR / \"requesta_mcqs_cleaned_parsed.xlsx\"\n",
    "ATTN_XLSX     = DIR / \"attention_checker_parsed.xlsx\"\n",
    "OUT_TXT       = DIR / \"all_questions_dict.txt\"\n",
    "\n",
    "# Deterministic randomness for attention insertion and sampling\n",
    "random.seed(42)\n",
    "\n",
    "LETTER_ID = {\"A\": \"a\", \"B\": \"b\", \"C\": \"c\", \"D\": \"d\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5271d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x)\n",
    "    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"\\u00A0\", \" \").strip()\n",
    "\n",
    "def answer_to_id(ans: Any) -> str | None:\n",
    "    if ans is None:\n",
    "        return None\n",
    "    m = re.search(r\"[A-Da-d]\", str(ans))\n",
    "    if not m:\n",
    "        return None\n",
    "    return LETTER_ID[m.group(0).upper()]\n",
    "\n",
    "# ---- Prefix options in DataFrames BEFORE downstream use ----\n",
    "def _ensure_letter_prefix(s: str, letter: str) -> str:\n",
    "    s = to_str(s)\n",
    "    if not s:\n",
    "        return s\n",
    "    if re.match(rf\"^{letter}\\)\\s\", s):  # already prefixed\n",
    "        return s\n",
    "    return f\"{letter}) {s}\"\n",
    "\n",
    "def prefix_options_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"optionA\" in df.columns:\n",
    "        df[\"optionA\"] = df[\"optionA\"].map(lambda x: _ensure_letter_prefix(x, \"A\"))\n",
    "    if \"optionB\" in df.columns:\n",
    "        df[\"optionB\"] = df[\"optionB\"].map(lambda x: _ensure_letter_prefix(x, \"B\"))\n",
    "    if \"optionC\" in df.columns:\n",
    "        df[\"optionC\"] = df[\"optionC\"].map(lambda x: _ensure_letter_prefix(x, \"C\"))\n",
    "    if \"optionD\" in df.columns:\n",
    "        df[\"optionD\"] = df[\"optionD\"].map(lambda x: _ensure_letter_prefix(x, \"D\"))\n",
    "    return df\n",
    "\n",
    "def build_choices_from_prefixed(row: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    # DataFrame options already include \"A) ...\", etc.\n",
    "    A = to_str(row.get(\"optionA\", \"\"))\n",
    "    B = to_str(row.get(\"optionB\", \"\"))\n",
    "    C = to_str(row.get(\"optionC\", \"\"))\n",
    "    D = to_str(row.get(\"optionD\", \"\"))\n",
    "    return [\n",
    "        {\"id\": \"a\", \"text\": A},\n",
    "        {\"id\": \"b\", \"text\": B},\n",
    "        {\"id\": \"c\", \"text\": C},\n",
    "        {\"id\": \"d\", \"text\": D},\n",
    "    ]\n",
    "\n",
    "def build_question(row: Dict[str, Any], answer_col: str) -> Dict[str, Any]:\n",
    "    # PROMPT now always comes from `question_item`\n",
    "    return {\n",
    "        \"question_id\": to_str(row.get(\"question_id\", \"\")),\n",
    "        \"prompt\": to_str(row.get(\"question_item\", \"\")),\n",
    "        \"choices\": build_choices_from_prefixed(row),\n",
    "        \"correct_choice_id\": answer_to_id(row.get(answer_col)),\n",
    "    }\n",
    "\n",
    "def insert_attention(qs: List[Dict[str, Any]], attn_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    if attn_df.empty:\n",
    "        return qs\n",
    "    idx = random.randrange(len(attn_df))\n",
    "    att_row = attn_df.iloc[idx].to_dict()\n",
    "    att_q = build_question(att_row, \"attention_answer\")\n",
    "    after_n = random.choice([2, 3, 4])\n",
    "    insert_at = min(after_n, len(qs))  # after Nth means index N (0-based); if list shorter, append\n",
    "    out = list(qs)\n",
    "    out.insert(insert_at, att_q)\n",
    "    return out\n",
    "\n",
    "def read_passages_dict(path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    text = path.read_text(encoding=\"utf-8\")   # PASSAGES is a plain dict literal {...}\n",
    "    passages = ast.literal_eval(text)\n",
    "    if not isinstance(passages, dict):\n",
    "        raise ValueError(\"PASSAGES text does not contain a dict literal.\")\n",
    "    return passages\n",
    "\n",
    "def read_excel_or_fail(path: Path, dtype: Dict[str, Any]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Excel file: {path}\")\n",
    "    return pd.read_excel(path, engine=\"openpyxl\", dtype=dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3885bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote QUESTION dict → some_recorded_output\\all_questions_dict.txt\n",
      "Passages: 20 | Baseline Qs (incl. attention): 115 | Requesta Qs (incl. attention): 115\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load inputs ----------\n",
    "PASSAGES = read_passages_dict(PASSAGES_TXT)\n",
    "\n",
    "def p_order(k: str) -> int:\n",
    "    m = re.match(r\"p(\\d+)$\", k)\n",
    "    return int(m.group(1)) if m else 10_000\n",
    "\n",
    "ordered_p_keys = sorted(PASSAGES.keys(), key=p_order)\n",
    "\n",
    "baseline_df = read_excel_or_fail(BASELINE_XLSX, dtype={\"textID\": str, \"question_id\": str})\n",
    "requesta_df = read_excel_or_fail(REQUESTA_XLSX, dtype={\"textID\": str, \"question_id\": str})\n",
    "attn_df     = read_excel_or_fail(ATTN_XLSX,     dtype={\"question_id\": str})\n",
    "\n",
    "# Keep only relevant columns (robust to extras)\n",
    "baseline_cols_keep = [c for c in baseline_df.columns if c in {\n",
    "    \"question_id\",\"textID\",\"baseline_question_type\",\"baseline_question\",\"baseline_answer\",\n",
    "    \"question_item\",\"optionA\",\"optionB\",\"optionC\",\"optionD\"\n",
    "}]\n",
    "requesta_cols_keep = [c for c in requesta_df.columns if c in {\n",
    "    \"question_id\",\"textID\",\"requesta_question_type\",\"requesta_question\",\"requesta_answer\",\n",
    "    \"question_item\",\"optionA\",\"optionB\",\"optionC\",\"optionD\"\n",
    "}]\n",
    "attn_cols_keep = [c for c in attn_df.columns if c in {\n",
    "    \"question_id\",\"attention_question\",\"attention_answer\",\"question_item\",\"optionA\",\"optionB\",\"optionC\",\"optionD\"\n",
    "}]\n",
    "\n",
    "baseline_df = prefix_options_df(baseline_df[baseline_cols_keep].copy())\n",
    "requesta_df = prefix_options_df(requesta_df[requesta_cols_keep].copy())\n",
    "attn_df     = prefix_options_df(attn_df[attn_cols_keep].copy())\n",
    "\n",
    "# ---------- Build QUESTION dict ----------\n",
    "QUESTION: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for pkey in ordered_p_keys:\n",
    "    text_id = to_str(PASSAGES[pkey].get(\"id\", \"\"))\n",
    "    b_rows = baseline_df.loc[baseline_df[\"textID\"] == text_id]\n",
    "    r_rows = requesta_df.loc[requesta_df[\"textID\"] == text_id]\n",
    "\n",
    "    baseline_qs: List[Dict[str, Any]] = [\n",
    "        build_question(row, \"baseline_answer\")\n",
    "        for row in b_rows.to_dict(orient=\"records\")\n",
    "    ]\n",
    "    requesta_qs: List[Dict[str, Any]] = [\n",
    "        build_question(row, \"requesta_answer\")\n",
    "        for row in r_rows.to_dict(orient=\"records\")\n",
    "    ]\n",
    "\n",
    "    # Insert one attention-check into each list (even if empty)\n",
    "    baseline_qs = insert_attention(baseline_qs, attn_df)\n",
    "    requesta_qs = insert_attention(requesta_qs, attn_df)\n",
    "\n",
    "    QUESTION[pkey] = {\n",
    "        \"id\": text_id,\n",
    "        \"questions\": {\n",
    "            \"baseline\": baseline_qs,\n",
    "            \"requesta\": requesta_qs,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ---------- Write output (dict literal only, UTF-8) ----------\n",
    "OUT_TXT.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_TXT.write_text(pformat(QUESTION, width=100, sort_dicts=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote QUESTION dict → {OUT_TXT}\")\n",
    "print(f\"Passages: {len(QUESTION)} | Baseline Qs (incl. attention): {sum(len(v['questions']['baseline']) for v in QUESTION.values())} \"\n",
    "      f\"| Requesta Qs (incl. attention): {sum(len(v['questions']['requesta']) for v in QUESTION.values())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
